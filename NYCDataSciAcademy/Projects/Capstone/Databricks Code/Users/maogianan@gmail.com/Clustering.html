<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>maogianan@gmail.com / Clustering - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"spark_heap_memory":21145,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26432,"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"spark_heap_memory":46131,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":57664,"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":96102,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":120128,"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":196044,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":245056,"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":8448,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10560,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":20736,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":25920,"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":45312,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":56640,"memory_mb":61440,"category":"Compute Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":21145,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26432,"memory_mb":31232,"category":"Storage Optimized","num_cores":4.0,"support_ebs_volumes":true},{"spark_heap_memory":46131,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":57664,"memory_mb":62464,"category":"Storage Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":96102,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":120128,"memory_mb":124928,"category":"Storage Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":196044,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":245056,"memory_mb":249856,"category":"Storage Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized","support_cluster_tags":false,"container_memory_mb":28000,"memory_mb":30720,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":false},{"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized","support_cluster_tags":false,"container_memory_mb":12128,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":false}],"default_node_type_id":"memory-optimized"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-east-1e","isDefault":true},{"id":"us-east-1b","isDefault":false},{"id":"us-east-1c","isDefault":false},{"id":"us-east-1d","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":false,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":true,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0 RC5 (Scala 2.11)","packageLabel":"spark-image-20833506f690f3a49c53fb08837cb9b98f7f2e15380f1fb26efc158d953e94c8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-4f22a8d3016bc3dce9e839b10418815a7d28afff3a027b43bd2b041c42b2a89d","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-596490ba3e0ca4b62abb048923fc70de84e319cf527bb7a5a8f609bbf780bed8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0 RC5 (Scala 2.10)","packageLabel":"spark-image-f430b652793cbc27e454dc629737ae2bc04b75a1fecd68a5d90c44eca7543756","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":false,"enableFeedback":true,"enableClusterAutoScaling":true,"enableUserVisibleDefaultTags":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":true,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":false,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.33.220","accountsLimit":-1,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":30,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":true,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"support@databricks.com","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-4f22a8d3016bc3dce9e839b10418815a7d28afff3a027b43bd2b041c42b2a89d","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"8719902676c4e1cefbc31755c1726979bdf286d0","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":true,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"enableTerminal":false,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":540,"name":"maogianan@gmail.com / Clustering","language":"python","commands":[{"version":"CommandV1","origId":542,"guid":"d6e59d75-200a-4e83-bbd8-b833668d3cf7","subtype":"command","commandType":"auto","position":1.0,"command":"# Run this cell to setup data path\nimport os\nfrom pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\ndatapath = os.getcwd()\nif datapath.find('databricks') != -1:\n  ACCESS_KEY =   \"AKIAIUOEPA4PZ2RIARXA\"\n  SECRET_KEY = \"vkVXcbOTaWx7k4HnrvaeWblnbZkedeYEX5DVtNN+\"\n  AWS_BUCKET_NAME = \"dummyvarsmillionsong\"\n  datapath = \"s3a://%s:%s@%s/\" %(ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME)\n\n# Load and parse ratings\ntripletsRDD = sc.textFile('processed_data/ratings_visible.txt')\nratings = tripletsRDD.map(lambda l: l.split(','))\\\n    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n  \n# Load and parse hidden ratings\ntriplets_hidden_RDD = sc.textFile('processed_data/ratings_hidden.txt')\nratings_hidden = triplets_hidden_RDD.map(lambda l: l.split(','))\\\n    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n\n# Sanity check\nprint \"number of ratings\", ratings.count()\nprint \"number of hidden ratings\", ratings_hidden.count()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">[Rating(user=65576, product=215801, rating=1.0), Rating(user=65576, product=322415, rating=1.0), Rating(user=65576, product=376377, rating=2.0)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18075.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18075.0 (TID 75143, 10.172.237.220): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-15-6004194681a4&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     17</span> ratings <span class=\"ansiyellow\">=</span> tripletsRDD<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> l<span class=\"ansiyellow\">:</span> l<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;,&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>    <span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> l<span class=\"ansiyellow\">:</span> Rating<span class=\"ansiyellow\">(</span>int<span class=\"ansiyellow\">(</span>l<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> int<span class=\"ansiyellow\">(</span>l<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> float<span class=\"ansiyellow\">(</span>l<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     18</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 19</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">print</span> ratings<span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">take</span><span class=\"ansiblue\">(self, num)</span>\n<span class=\"ansigreen\">   1308</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1309</span>             p <span class=\"ansiyellow\">=</span> range<span class=\"ansiyellow\">(</span>partsScanned<span class=\"ansiyellow\">,</span> min<span class=\"ansiyellow\">(</span>partsScanned <span class=\"ansiyellow\">+</span> numPartsToTry<span class=\"ansiyellow\">,</span> totalParts<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1310</span><span class=\"ansiyellow\">             </span>res <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> takeUpToNumLeft<span class=\"ansiyellow\">,</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1311</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1312</span>             items <span class=\"ansiyellow\">+=</span> res<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.pyc</span> in <span class=\"ansicyan\">runJob</span><span class=\"ansiblue\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansigreen\">    931</span>         <span class=\"ansired\"># SparkContext#runJob.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    932</span>         mappedRDD <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>partitionFunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 933</span><span class=\"ansiyellow\">         </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jsc<span class=\"ansiyellow\">.</span>sc<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">,</span> partitions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    934</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    935</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18075.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18075.0 (TID 75143, 10.172.237.220): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;&lt;ipython-input-15-6004194681a4&gt;&quot;, line 17, in &lt;lambda&gt;\nNameError: global name &apos;Rating&apos; is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor204.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;&lt;ipython-input-15-6004194681a4&gt;&quot;, line 17, in &lt;lambda&gt;\nNameError: global name &apos;Rating&apos; is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.481464803184E12,"submitTime":1.48146480277E12,"finishTime":1.481464806703E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1687c3a2-68d0-47f2-a9a9-3cce1347d974"},{"version":"CommandV1","origId":543,"guid":"0c6635a5-1691-4c66-beec-3313cba3841e","subtype":"command","commandType":"auto","position":2.0,"command":"# Implicit Recommendation\n\n# Build the recommendation model using Alternating Least Squares\nmodel = ALS.trainImplicit(ratings, rank = 10, iterations = 10, alpha = 40.0, lambda_ = 1.0)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481464813929E12,"submitTime":1.481464813547E12,"finishTime":1.481464820571E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ba02a21b-fa93-4e60-9526-bfdc057d6406"},{"version":"CommandV1","origId":550,"guid":"578ccf89-5364-4bd0-a0f9-b80b3c80653a","subtype":"command","commandType":"auto","position":4.0,"command":"# create userFeatures RDD\nuF = model.userFeatures()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;function&apos; object has no attribute &apos;collect&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-20-0f21796879fd&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansired\"># create userFeatures RDD</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span>uF <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>userFeatures<span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;function&apos; object has no attribute &apos;collect&apos;\n</div>","workflows":[],"startTime":1.481470960443E12,"submitTime":1.481470960265E12,"finishTime":1.481470960565E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"14092e9a-3717-470b-8db0-788070e93099"},{"version":"CommandV1","origId":553,"guid":"b2d2e402-41f8-4dbb-8309-2d633f31d4da","subtype":"command","commandType":"auto","position":5.5,"command":"%md # K-means Clustering","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b435e69e-849c-4373-b24b-3b175eb0fb23"},{"version":"CommandV1","origId":708,"guid":"ebd96309-d0af-43f7-a99c-21aa9ad5bee6","subtype":"command","commandType":"auto","position":5.625,"command":"# Configuration\nACCESS_KEY = \"AKIAIUOEPA4PZ2RIARXA\"\nSECRET_KEY = \"vkVXcbOTaWx7k4HnrvaeWblnbZkedeYEX5DVtNN+\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"dummyvarsmillionsong\"\nMOUNT_NAME = \"/S3/\"\n# Mount the S3 bucket\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n# Display contents of bucket\ndisplay(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ea2fac32-39dd-4455-ab23-4d78822795ea"},{"version":"CommandV1","origId":552,"guid":"5b6c4348-88be-4d0a-bf92-f163e7b56b81","subtype":"command","commandType":"auto","position":6.0,"command":"from pyspark.mllib.clustering import KMeans, KMeansModel\nfrom math import sqrt\n\n# Build the model (cluster the data)\n\nclusters = KMeans.train(uF.map(lambda l: l[1]), i, maxIterations=100, \\\n        runs=10, initializationMode=\"random\", seed=0)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors\ndef error(point):\n    center = clusters.centers[clusters.predict(point)]\n    return sqrt(sum([x**2 for x in (point - center)]))\n\nWSSSE = uF.map(lambda l: l[1]).map(lambda point: error(point)).reduce(lambda x, y: x + y)\nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Within Set Sum of Squared Error = 5069.95132411\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"py4j.Py4JException: Method trainKMeansModel([class org.apache.spark.api.java.JavaRDD, class java.lang.Double, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer, class java.lang.Integer, class java.lang.Double, class java.util.ArrayList]) does not exist","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-101-eb559f273914&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansired\"># Build the model (cluster the data)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 6</span><span class=\"ansiyellow\"> </span>clusters <span class=\"ansiyellow\">=</span> KMeans<span class=\"ansiyellow\">.</span>train<span class=\"ansiyellow\">(</span>uF<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> l<span class=\"ansiyellow\">:</span> l<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> i<span class=\"ansiyellow\">,</span> maxIterations<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">10</span><span class=\"ansiyellow\">,</span>         runs<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">10</span><span class=\"ansiyellow\">,</span> initializationMode<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;random&quot;</span><span class=\"ansiyellow\">,</span> seed<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span> <span class=\"ansired\"># # Evaluate clustering by computing Within Set Sum of Squared Errors</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/clustering.pyc</span> in <span class=\"ansicyan\">train</span><span class=\"ansiblue\">(cls, rdd, k, maxIterations, runs, initializationMode, seed, initializationSteps, epsilon, initialModel)</span>\n<span class=\"ansigreen\">    354</span>         model = callMLlibFunc(&quot;trainKMeansModel&quot;, rdd.map(_convert_to_vector), k, maxIterations,\n<span class=\"ansigreen\">    355</span>                               runs<span class=\"ansiyellow\">,</span> initializationMode<span class=\"ansiyellow\">,</span> seed<span class=\"ansiyellow\">,</span> initializationSteps<span class=\"ansiyellow\">,</span> epsilon<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 356</span><span class=\"ansiyellow\">                               clusterInitialModel)\n</span><span class=\"ansigreen\">    357</span>         centers <span class=\"ansiyellow\">=</span> callJavaFunc<span class=\"ansiyellow\">(</span>rdd<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">,</span> model<span class=\"ansiyellow\">.</span>clusterCenters<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    358</span>         <span class=\"ansigreen\">return</span> KMeansModel<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">[</span>c<span class=\"ansiyellow\">.</span>toArray<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> c <span class=\"ansigreen\">in</span> centers<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/common.pyc</span> in <span class=\"ansicyan\">callMLlibFunc</span><span class=\"ansiblue\">(name, *args)</span>\n<span class=\"ansigreen\">    128</span>     sc <span class=\"ansiyellow\">=</span> SparkContext<span class=\"ansiyellow\">.</span>getOrCreate<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    129</span>     api <span class=\"ansiyellow\">=</span> getattr<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonMLLibAPI<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> name<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 130</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> callJavaFunc<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> api<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">*</span>args<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    131</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    132</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/mllib/common.pyc</span> in <span class=\"ansicyan\">callJavaFunc</span><span class=\"ansiblue\">(sc, func, *args)</span>\n<span class=\"ansigreen\">    121</span>     <span class=\"ansiblue\">&quot;&quot;&quot; Call Java Function &quot;&quot;&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    122</span>     args <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>_py2java<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> a<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">for</span> a <span class=\"ansigreen\">in</span> args<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 123</span><span class=\"ansiyellow\">     </span><span class=\"ansigreen\">return</span> _java2py<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> func<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>args<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    124</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    125</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n<span class=\"ansigreen\">    322</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}. Trace:\\n{3}\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 323</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name, value))\n</span><span class=\"ansigreen\">    324</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    325</span>             raise Py4JError(\n\n<span class=\"ansired\">Py4JError</span>: An error occurred while calling o3760.trainKMeansModel. Trace:\npy4j.Py4JException: Method trainKMeansModel([class org.apache.spark.api.java.JavaRDD, class java.lang.Double, class java.lang.Integer, class java.lang.Integer, class java.lang.String, class java.lang.Integer, class java.lang.Integer, class java.lang.Double, class java.util.ArrayList]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n\tat py4j.Gateway.invoke(Gateway.java:272)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n\n</div>","workflows":[],"startTime":1.48140635272E12,"submitTime":1.481406349679E12,"finishTime":1.481406354831E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6a12eecb-ec3f-4633-a68e-fbf1c2a9c57d"},{"version":"CommandV1","origId":607,"guid":"bda93d05-9d26-43b1-89bc-0faea9d822ce","subtype":"command","commandType":"auto","position":6.25,"command":"# # Finding optimal k\n# clustering_error = []\n# for i in range(98,101):\n#   clusters = KMeans.train(uF.map(lambda l: l[1]), i, maxIterations=100, \\\n#         runs=10, initializationMode=\"random\", seed=0)\n#   def error(point):\n#     center = clusters.centers[clusters.predict(point)]\n#     return sqrt(sum([x**2 for x in (point - center)]))\n#   WSSSE = uF.map(lambda l: l[1]).map(lambda point: error(point)).reduce(lambda x, y: x + y)\n#   print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n#   clustering_error.append({\"clusters\":i, \"wssse\":WSSSE})\n# import pandas as pd\n# cluster_error_set = pd.DataFrame(clustering_error)\n# cluster_error_set\n# %matplotlib inline\n# import matplotlib as mpl\n# import matplotlib.pyplot as plt\n\n# cluster_error_set.plot(kind=\"line\",x=\"clusters\", y=\"wssse\", figsize=(16,12));","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Within Set Sum of Squared Error = 5085.04538994\nWithin Set Sum of Squared Error = 5085.12218077\nWithin Set Sum of Squared Error = 5069.95132411\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481406363297E12,"submitTime":1.481406360261E12,"finishTime":1.48140636933E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"c081da97-a09b-4635-bf03-445c4f3ab251"},{"version":"CommandV1","origId":613,"guid":"7d553d73-e642-4b47-a652-2c2dd44ab51e","subtype":"command","commandType":"auto","position":6.375,"command":"import pandas as pd\ncluster_error_set = pd.DataFrame(clustering_error)\nprint cluster_error_set\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">   clusters        wssse\n0        95  5094.545100\n1        96  5096.373788\n2        97  5091.305438\n3        98  5085.045390\n4        99  5085.122181\n5       100  5069.951324\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">ImportError</span>: IPython.kernel.zmq requires pyzmq &gt;= 2.1.11","error":"<div class=\"ansiout\">    clusters        wssse\n0          2  7578.311892\n1          4  6828.175799\n2          6  6622.300768\n3          8  6487.824512\n4         10  6346.243247\n5         12  6285.819994\n6         14  6200.751267\n7         16  6102.803147\n8         18  6054.748858\n9         20  5958.695865\n10        22  5938.608517\n11        24  5867.191601\n12        26  5814.172509\n13        28  5791.350173\n14        30  5745.219868\n15        32  5706.650788\n16        34  5673.691897\n17        36  5642.005406\n18        38  5616.199987\n19        40  5581.064971\n20        42  5567.338183\n21        44  5523.643069\n22        46  5528.675755\n23        48  5477.653921\n24        50  5471.446738\n25        52  5433.001912\n26        54  5421.640804\n27        56  5396.010778\n28        58  5373.162584\n29        60  5366.601598\n30        62  5341.059836\n31        64  5332.937479\n32        66  5305.595147\n33        68  5305.512771\n34        70  5278.449117\n35        72  5245.991752\n36        74  5236.018353\n37        76  5228.283366\n38        78  5204.933454\n39        80  5187.656711\n40        82  5180.175314\n41        84  5161.158994\n42        86  5142.438634\n43        88  5145.314909\n44        90  5121.459516\n45        92  5115.083686\n46        94  5112.653954\n47        96  5090.058523\n48        98  5077.323206\n49       100  5059.704574\n50       102  5040.403942\n51       104  5040.322812\n52       106  5028.698540\n53       108  5027.957889\n54       110  5009.713207\n55       112  4998.515674\n56       114  4989.133233\n57       116  4990.532842\n58       118  4965.219902\n<span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ImportError</span>                               Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-18-9ad5b7b06428&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansigreen\">import</span> matplotlib <span class=\"ansigreen\">as</span> mpl<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      6</span> <span class=\"ansigreen\">import</span> matplotlib<span class=\"ansiyellow\">.</span>pyplot <span class=\"ansigreen\">as</span> plt<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 7</span><span class=\"ansiyellow\"> </span>get_ipython<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>magic<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">u&apos;matplotlib inline&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      8</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      9</span> cluster_error_set<span class=\"ansiyellow\">.</span>plot<span class=\"ansiyellow\">(</span>kind<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;line&quot;</span><span class=\"ansiyellow\">,</span>x<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;clusters&quot;</span><span class=\"ansiyellow\">,</span> y<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;wssse&quot;</span><span class=\"ansiyellow\">,</span> figsize<span class=\"ansiyellow\">=</span><span class=\"ansiyellow\">(</span><span class=\"ansicyan\">16</span><span class=\"ansiyellow\">,</span><span class=\"ansicyan\">12</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">;</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc</span> in <span class=\"ansicyan\">magic</span><span class=\"ansiblue\">(self, arg_s)</span>\n<span class=\"ansigreen\">   2203</span>         magic_name<span class=\"ansiyellow\">,</span> _<span class=\"ansiyellow\">,</span> magic_arg_s <span class=\"ansiyellow\">=</span> arg_s<span class=\"ansiyellow\">.</span>partition<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos; &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2204</span>         magic_name <span class=\"ansiyellow\">=</span> magic_name<span class=\"ansiyellow\">.</span>lstrip<span class=\"ansiyellow\">(</span>prefilter<span class=\"ansiyellow\">.</span>ESC_MAGIC<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2205</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>run_line_magic<span class=\"ansiyellow\">(</span>magic_name<span class=\"ansiyellow\">,</span> magic_arg_s<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2206</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2207</span>     <span class=\"ansired\">#-------------------------------------------------------------------------</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc</span> in <span class=\"ansicyan\">run_line_magic</span><span class=\"ansiblue\">(self, magic_name, line)</span>\n<span class=\"ansigreen\">   2124</span>                 kwargs<span class=\"ansiyellow\">[</span><span class=\"ansiblue\">&apos;local_ns&apos;</span><span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">=</span> sys<span class=\"ansiyellow\">.</span>_getframe<span class=\"ansiyellow\">(</span>stack_depth<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>f_locals<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2125</span>             <span class=\"ansigreen\">with</span> self<span class=\"ansiyellow\">.</span>builtin_trap<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2126</span><span class=\"ansiyellow\">                 </span>result <span class=\"ansiyellow\">=</span> fn<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>args<span class=\"ansiyellow\">,</span><span class=\"ansiyellow\">**</span>kwargs<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2127</span>             <span class=\"ansigreen\">return</span> result<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2128</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;decorator-gen-105&gt;</span> in <span class=\"ansicyan\">matplotlib</span><span class=\"ansiblue\">(self, line)</span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/core/magic.pyc</span> in <span class=\"ansicyan\">&lt;lambda&gt;</span><span class=\"ansiblue\">(f, *a, **k)</span>\n<span class=\"ansigreen\">    191</span>     <span class=\"ansired\"># but it&apos;s overkill for just that one bit of state.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    192</span>     <span class=\"ansigreen\">def</span> magic_deco<span class=\"ansiyellow\">(</span>arg<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 193</span><span class=\"ansiyellow\">         </span>call <span class=\"ansiyellow\">=</span> <span class=\"ansigreen\">lambda</span> f<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>k<span class=\"ansiyellow\">:</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>k<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    194</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    195</span>         <span class=\"ansigreen\">if</span> callable<span class=\"ansiyellow\">(</span>arg<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/core/magics/pylab.pyc</span> in <span class=\"ansicyan\">matplotlib</span><span class=\"ansiblue\">(self, line)</span>\n<span class=\"ansigreen\">     78</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">     79</span>         args <span class=\"ansiyellow\">=</span> magic_arguments<span class=\"ansiyellow\">.</span>parse_argstring<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>matplotlib<span class=\"ansiyellow\">,</span> line<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 80</span><span class=\"ansiyellow\">         </span>gui<span class=\"ansiyellow\">,</span> backend <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>shell<span class=\"ansiyellow\">.</span>enable_matplotlib<span class=\"ansiyellow\">(</span>args<span class=\"ansiyellow\">.</span>gui<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     81</span>         self<span class=\"ansiyellow\">.</span>_show_matplotlib_backend<span class=\"ansiyellow\">(</span>args<span class=\"ansiyellow\">.</span>gui<span class=\"ansiyellow\">,</span> backend<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     82</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc</span> in <span class=\"ansicyan\">enable_matplotlib</span><span class=\"ansiblue\">(self, gui)</span>\n<span class=\"ansigreen\">   2941</span>                 gui<span class=\"ansiyellow\">,</span> backend <span class=\"ansiyellow\">=</span> pt<span class=\"ansiyellow\">.</span>find_gui_and_backend<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>pylab_gui_select<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2942</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 2943</span><span class=\"ansiyellow\">         </span>pt<span class=\"ansiyellow\">.</span>activate_matplotlib<span class=\"ansiyellow\">(</span>backend<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2944</span>         pt<span class=\"ansiyellow\">.</span>configure_inline_support<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> backend<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   2945</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/core/pylabtools.pyc</span> in <span class=\"ansicyan\">activate_matplotlib</span><span class=\"ansiblue\">(backend)</span>\n<span class=\"ansigreen\">    286</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    287</span>     <span class=\"ansigreen\">import</span> matplotlib<span class=\"ansiyellow\">.</span>pyplot<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 288</span><span class=\"ansiyellow\">     </span>matplotlib<span class=\"ansiyellow\">.</span>pyplot<span class=\"ansiyellow\">.</span>switch_backend<span class=\"ansiyellow\">(</span>backend<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    289</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    290</span>     <span class=\"ansired\"># This must be imported last in the matplotlib series, after</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/matplotlib/pyplot.pyc</span> in <span class=\"ansicyan\">switch_backend</span><span class=\"ansiblue\">(newbackend)</span>\n<span class=\"ansigreen\">    230</span>     matplotlib<span class=\"ansiyellow\">.</span>use<span class=\"ansiyellow\">(</span>newbackend<span class=\"ansiyellow\">,</span> warn<span class=\"ansiyellow\">=</span>False<span class=\"ansiyellow\">,</span> force<span class=\"ansiyellow\">=</span>True<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    231</span>     <span class=\"ansigreen\">from</span> matplotlib<span class=\"ansiyellow\">.</span>backends <span class=\"ansigreen\">import</span> pylab_setup<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 232</span><span class=\"ansiyellow\">     </span>_backend_mod<span class=\"ansiyellow\">,</span> new_figure_manager<span class=\"ansiyellow\">,</span> draw_if_interactive<span class=\"ansiyellow\">,</span> _show <span class=\"ansiyellow\">=</span> pylab_setup<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    233</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    234</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/matplotlib/backends/__init__.pyc</span> in <span class=\"ansicyan\">pylab_setup</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     30</span>     <span class=\"ansired\"># imports. 0 means only perform absolute imports.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     31</span>     backend_mod = __import__(backend_name,\n<span class=\"ansigreen\">---&gt; 32</span><span class=\"ansiyellow\">                              globals(),locals(),[backend_name],0)\n</span><span class=\"ansigreen\">     33</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     34</span>     <span class=\"ansired\"># Things we pull in from all backends</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/kernel/__init__.py</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansired\"># just for friendlier zmq version check</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">from</span> <span class=\"ansiyellow\">.</span> <span class=\"ansigreen\">import</span> zmq<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      6</span> <span class=\"ansigreen\">from</span> <span class=\"ansiyellow\">.</span>connect <span class=\"ansigreen\">import</span> <span class=\"ansiyellow\">*</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/kernel/zmq/__init__.py</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     12</span> <span class=\"ansigreen\">from</span> IPython<span class=\"ansiyellow\">.</span>utils<span class=\"ansiyellow\">.</span>zmqrelated <span class=\"ansigreen\">import</span> check_for_zmq<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 14</span><span class=\"ansiyellow\"> </span>check_for_zmq<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;2.1.11&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;IPython.kernel.zmq&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     16</span> <span class=\"ansigreen\">from</span> <span class=\"ansiyellow\">.</span>session <span class=\"ansigreen\">import</span> Session<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/python/local/lib/python2.7/site-packages/IPython/utils/zmqrelated.pyc</span> in <span class=\"ansicyan\">check_for_zmq</span><span class=\"ansiblue\">(minimum_version, required_by)</span>\n<span class=\"ansigreen\">     35</span>         <span class=\"ansigreen\">import</span> zmq<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     36</span>     <span class=\"ansigreen\">except</span> ImportError<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 37</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">raise</span> ImportError<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;%s requires pyzmq &gt;= %s&quot;</span><span class=\"ansiyellow\">%</span><span class=\"ansiyellow\">(</span>required_by<span class=\"ansiyellow\">,</span> minimum_version<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     38</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     39</span>     patch_pyzmq<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ImportError</span>: IPython.kernel.zmq requires pyzmq &gt;= 2.1.11\n</div>","workflows":[],"startTime":1.481406070588E12,"submitTime":1.481406063189E12,"finishTime":1.481406070658E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6cd613c6-098b-4bae-bd1c-b65d8b16cdb6"},{"version":"CommandV1","origId":600,"guid":"243f1661-8cfe-487d-b7bf-b7741c1ae6c2","subtype":"command","commandType":"auto","position":8.0,"command":"# Create userID:CLuster table\npointClusters = uF.map(lambda l: (l[0], clusters.predict(l[1])))\npointClusters.take(3)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">47</span><span class=\"ansired\">]: </span>[(1408, 80), (2304, 56), (2496, 57)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481401831181E12,"submitTime":1.481401828261E12,"finishTime":1.481401831302E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0d488b00-6c75-4c6d-8bf0-682cb4e72111"},{"version":"CommandV1","origId":614,"guid":"980fc051-e81f-4f0f-9a31-4772e8319354","subtype":"command","commandType":"auto","position":9.0,"command":"pointClusters.repartition(1).saveAsTextFile(\"/mnt/S3/processed_data/userID_Clusters.txt\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/mnt/S3/processed_data/userID_Clusters.txt already exists","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-48-378c058b0227&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>pointClusters<span class=\"ansiyellow\">.</span>repartition<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>saveAsTextFile<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/mnt/S3/processed_data/userID_Clusters.txt&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">saveAsTextFile</span><span class=\"ansiblue\">(self, path, compressionCodecClass)</span>\n<span class=\"ansigreen\">   1517</span>             keyed<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>BytesToString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>saveAsTextFile<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">,</span> compressionCodec<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1518</span>         <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1519</span><span class=\"ansiyellow\">             </span>keyed<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>BytesToString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>saveAsTextFile<span class=\"ansiyellow\">(</span>path<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1520</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1521</span>     <span class=\"ansired\"># Pair functions</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o2334.saveAsTextFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory dbfs:/mnt/S3/processed_data/userID_Clusters.txt already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1184)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1161)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1064)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1030)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:956)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:956)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:955)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1459)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1438)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1438)\n\tat org.apache.spark.api.java.JavaRDDLike$class.saveAsTextFile(JavaRDDLike.scala:549)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n</div>","workflows":[],"startTime":1.481401871392E12,"submitTime":1.481401867985E12,"finishTime":1.481401874405E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a999f78f-29a0-45c0-8c04-c9c1e8b8d41c"},{"version":"CommandV1","origId":623,"guid":"77416b73-0c2f-42c8-b99b-52ff6741b21f","subtype":"command","commandType":"auto","position":10.0,"command":"#Function version\n\nfrom pyspark.mllib.clustering import KMeans, KMeansModel\nimport math\n\ndef create_cluster_assign(model):\n  \"\"\"\n  K-means clustering\n  Function takes in model created from ALS training.\n  Creates k-means cluster grouping all userIDs into k clusters where k = sqrt(# of unique users)\n  Will output a list of userID and cluster number assignment pair\n  \"\"\"\n  uF = model.userFeatures()\n  i = int(math.sqrt(uF.count()))\n# Build the model (cluster the data)\n  clusters = KMeans.train(uF.map(lambda l: l[1]), i, maxIterations=100,\\\n                          initializationMode=\"random\", seed=0)\n  pointCluster = uF.map(lambda l: (l[0], clusters.predict(l[1])))\n  return pointCluster","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481465003284E12,"submitTime":1.481465002906E12,"finishTime":1.481465003323E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"a3e6fff3-ca51-4c46-be26-ad3547920594"},{"version":"CommandV1","origId":639,"guid":"753308ed-f333-43c1-a9f7-263a95ac3c01","subtype":"command","commandType":"auto","position":11.0,"command":"c_list = create_cluster_assign(model)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: global name &apos;math&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-27-52ab602ac797&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>c_list <span class=\"ansiyellow\">=</span> create_cluster_assign<span class=\"ansiyellow\">(</span>model<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-26-497471d85bc1&gt;</span> in <span class=\"ansicyan\">create_cluster_assign</span><span class=\"ansiblue\">(model)</span>\n<span class=\"ansigreen\">     12</span>   &quot;&quot;&quot;\n<span class=\"ansigreen\">     13</span>   uF <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>userFeatures<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 14</span><span class=\"ansiyellow\">   </span>i <span class=\"ansiyellow\">=</span> int<span class=\"ansiyellow\">(</span>math<span class=\"ansiyellow\">.</span>sqrt<span class=\"ansiyellow\">(</span>uF<span class=\"ansiyellow\">.</span>count<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansired\"># Build the model (cluster the data)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     16</span>   clusters <span class=\"ansiyellow\">=</span> KMeans<span class=\"ansiyellow\">.</span>train<span class=\"ansiyellow\">(</span>uF<span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> l<span class=\"ansiyellow\">:</span> l<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> i<span class=\"ansiyellow\">,</span> maxIterations<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">100</span><span class=\"ansiyellow\">,</span>                          initializationMode<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">&quot;random&quot;</span><span class=\"ansiyellow\">,</span> seed<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: global name &apos;math&apos; is not defined\n</div>","workflows":[],"startTime":1.481470995196E12,"submitTime":1.481470995012E12,"finishTime":1.48147100043E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3edb73b7-d5b7-4dec-9e08-56488f9388a1"},{"version":"CommandV1","origId":641,"guid":"5cad1b64-9996-41ea-ad12-cb77648c2d33","subtype":"command","commandType":"auto","position":13.0,"command":"%md # Create Groups","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6c47d6fa-3122-436e-938b-025b73955bab"},{"version":"CommandV1","origId":643,"guid":"34054b3d-168e-4ad1-ac83-de3a1bd525be","subtype":"command","commandType":"auto","position":14.0,"command":"# # Create homogenous group\nimport random\ndef create_ho_grp(n_mem, c_list):\n  \"\"\"\n  Create homogenous group\n  Function takes in list created from create_cluster_assign function\n  This creates a list of userID and cluster number assignment pairs\n  where userIDs are from the same cluster.\n  where n_mem is the number of members per group.\n  \"\"\"\n  r = random.randrange(c_list.map(lambda p: p[1]).max())\n  group = c_list.filter(lambda p: p[1] == r).takeSample(False, n_mem)\n  return group\n\n# # Create heterogenous group\ndef create_he_grp(n_mem, c_list):\n  \"\"\"\n  Create heterogenous group\n  Function takes in list created from create_cluster_assign function\n  This creates a list of userID and cluster number assignment pairs\n  where userIDs are from different clusters.\n  where n_mem is the number of members per group.\n  \"\"\"\n  pop = c_list.map(lambda p: p[1]).distinct().collect()\n  g = random.sample(pop,n_mem)\n  group = []\n  for m in g:\n    group.append(c_list.filter(lambda p: p[1] == m).takeSample(False, 1))\n  return group","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">IndentationError</span><span class=\"ansired\">:</span> unindent does not match any outer indentation level","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-218-c6bd4fca4be3&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">16</span>\n<span class=\"ansiyellow\">    return group</span>\n<span class=\"ansigrey\">                ^</span>\n<span class=\"ansired\">IndentationError</span><span class=\"ansired\">:</span> unindent does not match any outer indentation level\n\nIf you want to paste code into IPython, try the %paste and %cpaste magic functions.\n</div>","workflows":[],"startTime":1.481471023459E12,"submitTime":1.481471023269E12,"finishTime":1.481471023534E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ac472673-4934-4b0f-a910-4801ef7ddfff"},{"version":"CommandV1","origId":666,"guid":"4e738e08-b006-4b1b-a015-70158b62f9ef","subtype":"command","commandType":"auto","position":16.0,"command":"grp = create_ho_grp(3, c_list)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;c_list&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-13-4ebeb6535711&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>grp <span class=\"ansiyellow\">=</span> create_ho_grp<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">,</span> c_list<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;c_list&apos; is not defined\n</div>","workflows":[],"startTime":1.481471027352E12,"submitTime":1.481471027166E12,"finishTime":1.481471033179E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4297b87c-a716-401b-9167-749fe55dd9a9"}],"dashboards":[],"guid":"c142690f-c0f5-489d-8064-3fbf100841bf","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"736a8ec3-02ca-40ba-b3cb-e59abb4c601b","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>