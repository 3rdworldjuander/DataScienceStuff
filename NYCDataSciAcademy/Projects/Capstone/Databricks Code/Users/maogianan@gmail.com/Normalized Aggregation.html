<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>maogianan@gmail.com / Normalized Aggregation - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"spark_heap_memory":21145,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26432,"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"spark_heap_memory":46131,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":57664,"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":96102,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":120128,"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":196044,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":245056,"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":8448,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10560,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":20736,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":25920,"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":45312,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":56640,"memory_mb":61440,"category":"Compute Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":21145,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26432,"memory_mb":31232,"category":"Storage Optimized","num_cores":4.0,"support_ebs_volumes":true},{"spark_heap_memory":46131,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":57664,"memory_mb":62464,"category":"Storage Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":96102,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":120128,"memory_mb":124928,"category":"Storage Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":196044,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":245056,"memory_mb":249856,"category":"Storage Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized","support_cluster_tags":false,"container_memory_mb":28000,"memory_mb":30720,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":false},{"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized","support_cluster_tags":false,"container_memory_mb":12128,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":false}],"default_node_type_id":"memory-optimized"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-east-1e","isDefault":true},{"id":"us-east-1b","isDefault":false},{"id":"us-east-1c","isDefault":false},{"id":"us-east-1d","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":false,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":true,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0 RC5 (Scala 2.11)","packageLabel":"spark-image-20833506f690f3a49c53fb08837cb9b98f7f2e15380f1fb26efc158d953e94c8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-4f22a8d3016bc3dce9e839b10418815a7d28afff3a027b43bd2b041c42b2a89d","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-596490ba3e0ca4b62abb048923fc70de84e319cf527bb7a5a8f609bbf780bed8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0 RC5 (Scala 2.10)","packageLabel":"spark-image-f430b652793cbc27e454dc629737ae2bc04b75a1fecd68a5d90c44eca7543756","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":false,"enableFeedback":true,"enableClusterAutoScaling":true,"enableUserVisibleDefaultTags":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":true,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":false,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.33.220","accountsLimit":-1,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":30,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":true,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"support@databricks.com","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-4f22a8d3016bc3dce9e839b10418815a7d28afff3a027b43bd2b041c42b2a89d","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"8719902676c4e1cefbc31755c1726979bdf286d0","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":true,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"enableTerminal":false,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":941,"name":"maogianan@gmail.com / Normalized Aggregation","language":"python","commands":[{"version":"CommandV1","origId":948,"guid":"368b37fb-7c9b-45e6-9df0-f1f9dd08fdc3","subtype":"command","commandType":"auto","position":5.625,"command":"# Mounting S3 bucket\nimport os\nfrom pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\n# S3 bucket mounting \nACCESS_KEY = \"\"\nSECRET_KEY = \"+\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"dummyvarsmillionsong\"\nMOUNT_NAME = \"/S3/\"\n# Mount the S3 bucket\n# dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n# Display contents of bucket\ndisplay(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/mnt/S3/mapping/","mapping/",0.0],["dbfs:/mnt/S3/models/","models/",0.0],["dbfs:/mnt/S3/processed_data/","processed_data/",0.0],["dbfs:/mnt/S3/raw_data/","raw_data/",0.0],["dbfs:/mnt/S3/results/","results/",0.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":"java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/S3; nested exception is: ","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">ExecutionError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-1-926f757da769&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     10</span> MOUNT_NAME <span class=\"ansiyellow\">=</span> <span class=\"ansiblue\">&quot;/S3/&quot;</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     11</span> <span class=\"ansired\"># Mount the S3 bucket</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 12</span><span class=\"ansiyellow\"> </span>dbutils<span class=\"ansiyellow\">.</span>fs<span class=\"ansiyellow\">.</span>mount<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;s3a://%s:%s@%s&quot;</span> <span class=\"ansiyellow\">%</span> <span class=\"ansiyellow\">(</span>ACCESS_KEY<span class=\"ansiyellow\">,</span> ENCODED_SECRET_KEY<span class=\"ansiyellow\">,</span> AWS_BUCKET_NAME<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;/mnt/%s&quot;</span> <span class=\"ansiyellow\">%</span> MOUNT_NAME<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     13</span> <span class=\"ansired\"># Display contents of bucket</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> display<span class=\"ansiyellow\">(</span>dbutils<span class=\"ansiyellow\">.</span>fs<span class=\"ansiyellow\">.</span>ls<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/mnt/%s&quot;</span> <span class=\"ansiyellow\">%</span> MOUNT_NAME<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/tmp/1481646737113-0/dbutils.pyc</span> in <span class=\"ansicyan\">f_with_exception_handling</span><span class=\"ansiblue\">(*args, **kwargs)</span>\n<span class=\"ansigreen\">    156</span>                     <span class=\"ansigreen\">class</span> ExecutionError<span class=\"ansiyellow\">(</span>Exception<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    157</span>                         <span class=\"ansigreen\">pass</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 158</span><span class=\"ansiyellow\">                     </span><span class=\"ansigreen\">raise</span> ExecutionError<span class=\"ansiyellow\">(</span>str<span class=\"ansiyellow\">(</span>e<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    159</span>             <span class=\"ansigreen\">return</span> f_with_exception_handling<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    160</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">ExecutionError</span>: An error occurred while calling o93.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/S3; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/S3\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:89)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:47)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:306)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/S3\n\tat scala.Predef$.require(Predef.scala:233)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:121)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:38)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:63)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext$$anonfun$queryHandlers$1.apply(SessionContext.scala:62)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:62)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:221)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend$$anonfun$receive$1.applyOrElse(DbfsServerBackend.scala:202)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:41)\n\tat com.databricks.rpc.ServerBackend$$anonfun$internalReceive$1.applyOrElse(ServerBackend.scala:36)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:56)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:56)\n\tat scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$10.apply(JettyServer.scala:263)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:263)\n\tat com.databricks.rpc.JettyServer$RequestManager.com$databricks$rpc$JettyServer$RequestManager$$handleRequestAndRespond(JettyServer.scala:200)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply$mcV$sp(JettyServer.scala:145)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:136)\n\tat com.databricks.rpc.JettyServer$RequestManager$$anonfun$handleHttp$1.apply(JettyServer.scala:136)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:145)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:140)\n\tat com.databricks.rpc.JettyServer$.withAttributionContext(JettyServer.scala:75)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:178)\n\tat com.databricks.rpc.JettyServer$.withAttributionTags(JettyServer.scala:75)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:135)\n\tat com.databricks.rpc.JettyServer$RequestManager.doGet(JettyServer.scala:90)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:687)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:790)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:845)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:583)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:524)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:319)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:253)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:273)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:95)\n\tat org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)\n\tat org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.482028496968E12,"submitTime":1.482028495555E12,"finishTime":1.482028502002E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ea2fac32-39dd-4455-ab23-4d78822795ea"},{"version":"CommandV1","origId":949,"guid":"dd8f2d67-2b9b-4398-8168-2d54361d4b3c","subtype":"command","commandType":"auto","position":7.8125,"command":"from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\n# Load model from S3\nmodel = MatrixFactorizationModel.load(sc, \"/mnt/S3/models/small_model\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;MatrixFactorizationModel&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-5-f1aefdc2d243&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      6</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansired\"># Load model from S3</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 8</span><span class=\"ansiyellow\"> </span>model <span class=\"ansiyellow\">=</span> MatrixFactorizationModel<span class=\"ansiyellow\">.</span>load<span class=\"ansiyellow\">(</span>sc<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;/mnt/S3/models/full_model&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;MatrixFactorizationModel&apos; is not defined\n</div>","workflows":[],"startTime":1.482028504189E12,"submitTime":1.482028502822E12,"finishTime":1.482028517247E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d7bd3fa3-4230-45ec-90d4-a92cbab25d2d"},{"version":"CommandV1","origId":1131,"guid":"6f6c8607-dfb9-4ec1-bd4f-40deb29cf86f","subtype":"command","commandType":"auto","position":12.90625,"command":"%md K means and Group creation","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ce13e34e-1b38-4f3c-9e4d-1668d6e6420f"},{"version":"CommandV1","origId":1132,"guid":"bd50842f-96e2-4448-8c70-cb547fd74056","subtype":"command","commandType":"auto","position":15.453125,"command":"# K-means clustering to assign users to groups\nfrom pyspark.mllib.clustering import KMeans, KMeansModel\nimport math\nimport random\nfrom operator import add\nimport numpy as np\n\ndef create_cluster_assign(model):\n  \"\"\"\n  K-means clustering\n  Function takes in model created from ALS training.\n  Creates k-means cluster grouping all userIDs into k clusters where k = sqrt(# of unique users)\n  Will output a list of userID and cluster number assignment pair\n  \"\"\"\n  uF = model.userFeatures()\n  i = int(math.sqrt(uF.count()))\n# Build the model (cluster the data)\n  clusters = KMeans.train(uF.map(lambda l: l[1]), i, maxIterations=100,\\\n                          initializationMode=\"random\", seed=0)\n  pointCluster = uF.map(lambda l: (l[0], clusters.predict(l[1])))\n  return pointCluster","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481931963213E12,"submitTime":1.481931961736E12,"finishTime":1.481931963283E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b0485e17-768c-4356-8730-e8869e22ee84"},{"version":"CommandV1","origId":1134,"guid":"acd1bede-321b-4fa4-97b0-b149c29acfc1","subtype":"command","commandType":"auto","position":16.08984375,"command":"# Create list of users and cluster assignment\nc_list = create_cluster_assign(model)\n\n# Save List\n# c_list.repartition(1).saveAsTextFile(\"/mnt/S3/processed_data/userID_Clusters_good.txt\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481931970885E12,"submitTime":1.481931969431E12,"finishTime":1.481931981436E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"37c66fb7-a10c-41d4-95eb-04471362a4e0"},{"version":"CommandV1","origId":1135,"guid":"0a5fb127-a39d-470f-a1b9-ed40141dcade","subtype":"command","commandType":"auto","position":16.408203125,"command":"# # Create homogenous group\nimport random\nfrom itertools import chain\ndef create_ho_grp(n_mem, c_list):\n  \"\"\"\n  Create homogenous group\n  Function takes in list created from create_cluster_assign function\n  This creates a list of userID and cluster number assignment pairs\n  where userIDs are from the same cluster.\n  where n_mem is the number of members per group.\n  \"\"\"\n  \n  # The max() part is taking too long for the full dataset: r = random.randrange(c_list.map(lambda p: p[1]).max())\n  r = random.randrange(100) # workaround for speed\n  # The takeSample is taking too long for the full dataset: group = c_list.filter(lambda p: p[1] == r).takeSample(False, n_mem)\n  group = c_list.filter(lambda p: p[1] == r).take(n_mem)\n  return group\n\n# # Create heterogenous group\ndef create_he_grp(n_mem, c_list):\n  \"\"\"\n  Create heterogenous group\n  Function takes in list created from create_cluster_assign function\n  This creates a list of userID and cluster number assignment pairs\n  where userIDs are from different clusters.\n  where n_mem is the number of members per group.\n  \"\"\"\n  # The distinct() part is taking too long for the full dataset: pop = c_list.map(lambda p: p[1]).distinct().collect()\n  pop = range(1, 100) # workaround for speed\n  g = random.sample(pop,n_mem)\n  group = []\n  for m in g:\n#  The takeSample is taking too long for the full dataset:   group.append(c_list.filter(lambda p: p[1] == m).takeSample(False, 1))\n    group.append(c_list.filter(lambda p: p[1] == m).take(1)) # workaround for speed\n  return list(chain(*group))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481931984309E12,"submitTime":1.48193198283E12,"finishTime":1.481931984379E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"54224a42-7128-4f8d-87e8-6e85b1aa1938"},{"version":"CommandV1","origId":1136,"guid":"feb0991d-e42b-4275-8fea-732de9e7aab9","subtype":"command","commandType":"auto","position":16.5673828125,"command":"# Function for creating multiple groups\n# this will be used for the statistical analysis of Aggregation Strategies\n\ndef create_many_grps(n_grps, n_mem):\n  ho_grps = []\n  he_grps = []\n  for i in range(n_grps):\n    ho_grps.append(create_ho_grp(n_mem, c_list))\n  for i in range(n_grps):\n    he_grps.append(create_he_grp(n_mem, c_list))\n  return ho_grps, he_grps\n    ","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481931995249E12,"submitTime":1.481931993722E12,"finishTime":1.481931995286E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"27688cb7-7218-4982-bd39-e9ecdde450e1"},{"version":"CommandV1","origId":1137,"guid":"bfd42c0e-8059-4fb1-a008-6e9f8512c60b","subtype":"command","commandType":"auto","position":16.64697265625,"command":"# ho_grps_2, he_grps_2 = create_many_grps(20,2)\nho_grps_3, he_grps_3 = create_many_grps(5,3)\nho_grps_5, he_grps_5 = create_many_grps(5,5)\nho_grps_7, he_grps_7 = create_many_grps(5,7)\n\nprint \"ho_grps_3\", ho_grps_3\nprint \"he_grps_3\", he_grps_3\nprint \"ho_grps_5\", ho_grps_5\nprint \"he_grps_5\", he_grps_5\nprint \"ho_grps_7\", ho_grps_7\nprint \"he_grps_7\", he_grps_7\n\n### GROUPINGS HERE!!! ###","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">ho_grps_3 [[(1167964, 18), (7812754, 18), (11031826, 18)], [(2373298, 88), (9102322, 88), (19312210, 88)], [(1827292, 48), (2350876, 48), (9797404, 48)], [(3638828, 66), (6534700, 66), (7591564, 66)], [(4900924, 94), (12570460, 94), (13481884, 94)]]\nhe_grps_3 [[(14664796, 4), (1051612, 45), (5220892, 70)], [(5492380, 98), (9845884, 99), (14140, 39)], [(72316, 79), (6946780, 78), (947986, 26)], [(12765592, 83), (4677916, 58), (10311292, 86)], [(4978492, 87), (1827292, 48), (3216850, 14)]]\nho_grps_5 [[(256540, 1), (4571260, 1), (9205948, 1), (3507730, 1), (12699538, 1)], [(7790332, 75), (18911644, 75), (7303916, 75), (7953548, 75), (9824876, 75)], [(8362396, 65), (17971132, 65), (7192210, 65), (7327954, 65), (8811442, 65)], [(6946780, 78), (15867100, 78), (5384108, 78), (1590346, 78), (1638826, 78)], [(4387036, 97), (14315740, 97), (9431986, 97), (9577426, 97), (10799122, 97)]]\nhe_grps_5 [[(14140, 39), (2373298, 88), (1846684, 90), (16419772, 82), (12612274, 5)], [(3078076, 15), (2777500, 69), (3087772, 55), (9390172, 11), (5220892, 70)], [(3638828, 66), (3194428, 24), (3465916, 80), (19530572, 25), (1846684, 90)], [(1604284, 71), (49086, 64), (4978492, 87), (2719324, 16), (4852444, 63)], [(7266748, 50), (10311292, 86), (17517642, 2), (5909308, 10), (947986, 26)]]\nho_grps_7 [[(4648828, 62), (7334620, 62), (13908508, 62), (14742364, 62), (19968508, 62), (5757202, 62), (9131410, 62)], [(1187356, 47), (5779422, 47), (16037790, 47), (8351286, 47), (10450470, 47), (18352710, 47), (8578940, 47)], [(1167964, 18), (7812754, 18), (11031826, 18), (5471372, 18), (17009612, 18), (16086270, 18), (3248362, 18)], [(6277756, 93), (7014652, 93), (12880732, 93), (104434, 93), (317746, 93), (4108882, 93), (5679634, 93)], [(8362396, 65), (17971132, 65), (7192210, 65), (7327954, 65), (8811442, 65), (16403410, 65), (10047884, 65)]]\nhe_grps_7 [[(4677916, 58), (947986, 26), (2373298, 88), (5220892, 70), (5773564, 8), (3638828, 66), (19530572, 25)], [(17623894, 81), (2777500, 69), (1187356, 47), (7266748, 50), (14664796, 4), (9390172, 11), (15033244, 33)], [(14140, 39), (11606314, 51), (14548444, 40), (3178066, 84), (19530572, 25), (17156668, 12), (2874460, 77)], [(947986, 26), (4377340, 35), (14819932, 67), (3178066, 84), (11085356, 89), (5337244, 74), (334108, 31)], [(343804, 34), (3216850, 14), (5388754, 6), (13304730, 20), (9681052, 96), (2835676, 46), (12612274, 5)]]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481937102622E12,"submitTime":1.48193710098E12,"finishTime":1.481937164929E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4cf44e24-6a8d-42b8-980b-80304723215b"},{"version":"CommandV1","origId":1138,"guid":"5b71cee2-b774-4775-ae73-4e05aedf471b","subtype":"command","commandType":"auto","position":16.686767578125,"command":"\nho_grps_3 = [ \\\n[(5220892, 70), (5715388, 70), (6723772, 70)], \\\n[(12612274, 5), (12079802, 5), (3712760, 5)], \\\n[(1827292, 48), (2350876, 48), (9797404, 48)], \\\n[(6277756, 93), (7014652, 93), (12880732, 93)], \\\n[(17156668, 12), (19037692, 12), (13017688, 12)]]\n\nhe_grps_3 = [ \\\n[(3465916, 80), (10311292, 86), (14580562, 54)], \\\n[(8362396, 65), (3194428, 24), (4648828, 62)], \\\n[(13976380, 76), (2777500, 69), (7298058, 72)], \\\n[(3638828, 66), (3078076, 15), (1846684, 90)], \\\n[(15091420, 60), (17623894, 81), (1167964, 18)]]\n\nho_grps_5 = [ \\\n[(3087772, 55), (13106770, 55), (17954770, 55), (14837708, 55), (8470264, 55)], \\\n[(2179378, 92), (13484914, 92), (4647212, 92), (15341900, 92), (8421784, 92)], \\\n[(1138876, 37), (3630748, 37), (4231900, 37), (3934354, 37), (4390066, 37)], \\\n[(1846684, 90), (3553180, 90), (10902748, 90), (11494204, 90), (16303420, 90)], \\\n[(3029596, 59), (7741852, 59), (8110300, 59), (13995772, 59), (3061714, 59)]]\n\nhe_grps_5 = [ \\\n[(15033244, 33), (1294012, 95), (3669532, 29), (15091420, 60), (3194428, 24)], \\\n[(1497628, 9), (5036668, 41), (7260082, 19), (15033244, 33), (11085356, 89)], \\\n[(334108, 31), (15033244, 33), (3078076, 15), (9845884, 99), (9390172, 11)], \\\n[(12765592, 83), (5773564, 8), (1439452, 17), (4387036, 97), (17517642, 2)], \\\n[(7790332, 75), (7072828, 13), (11085356, 89), (14364220, 44), (7260082, 19)]]\n\nho_grps_7 = [ \\\n[(3465916, 80), (9021724, 80), (10068892, 80), (14752060, 80), (17282716, 80), (7788716, 80), (9660044, 80)], \\\n[(6946780, 78), (15867100, 78), (5384108, 78), (1590346, 78), (1638826, 78), (15135052, 78), (4200388, 78)], \\\n[(16419772, 82), (14561170, 82), (11618636, 82), (17012440, 82), (2269470, 82), (9056670, 82), (17249790, 82)], \\\n[(15778220, 57), (2323000, 57), (18393918, 57), (5546314, 57), (17986282, 57), (4595500, 57), (8922340, 57)], \\\n[(4377340, 35), (10524604, 35), (8976274, 35), (18022642, 35), (3541868, 35), (16149496, 35), (20590264, 35)]]\n\nhe_grps_7 = [ \\\n[(13637020, 27), (17623894, 81), (11404314, 49), (3669532, 29), (4377340, 35), (2179378, 92), (10311292, 86)], \\\n[(9390172, 11), (19530572, 25), (5492380, 98), (11404314, 49), (4387036, 97), (8362396, 65), (5783260, 22)], \\\n[(10311292, 86), (2835676, 46), (6946780, 78), (12765592, 83), (2719324, 16), (1497628, 9), (9681052, 96)], \\\n[(1167964, 18), (1294012, 95), (5385724, 52), (11404314, 49), (3795580, 23), (14819932, 67), (7298058, 72)], \\\n[(5220892, 70), (2874460, 77), (15033244, 33), (1846684, 90), (5492380, 98), (19357660, 43), (17517642, 2)]]\n\n\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> unexpected character after line continuation character","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-26-96d91f37c60d&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">10</span>\n<span class=\"ansiyellow\">    he_grps_3 = [[(15033244, 33), (1294012, 95), (1604284, 71)], [(2179378, 92), (1827292, 48), (3216850, 14)], [(1167964, 18), (14548444, 40), (15778220, 57)], \\</span>\n<span class=\"ansigrey\">                                                                                                                                                                   ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> unexpected character after line continuation character\n\n</div>","workflows":[],"startTime":1.482028520168E12,"submitTime":1.482028518797E12,"finishTime":1.482028520247E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b0a0e094-f05a-496c-87f3-ac0c43936021"},{"version":"CommandV1","origId":1133,"guid":"968d8fbe-a390-491f-b385-4d39f64bce94","subtype":"command","commandType":"auto","position":16.7265625,"command":"%md END OF K means and group creation","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"976a7e42-2d7e-4d6f-85e5-86321148ec0e"},{"version":"CommandV1","origId":960,"guid":"a7b1559c-8f5f-480a-93fd-ae1016f74400","subtype":"command","commandType":"auto","position":18.0,"command":"%md # Aggregation Strategies","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4fb4e8b9-9031-4385-92cd-a2185e9c362c"},{"version":"CommandV1","origId":961,"guid":"052b43c1-1093-4030-bdab-3f3a5a6c9d59","subtype":"command","commandType":"auto","position":18.5,"command":"%md\n- get all song recommendations for all users. This assumes that reco_list is a list of songs unheard of by the user.  \n- using group function, create groups.  \n- get song_reco_list of each user\n- get intersection of all song_reco_list for the group\n- employ different Aggregation Strategies:  \n    - Least misery (get min) - DONE\n    - Average (get average) - DONE\n    - Most pleasure  (get max) - DONE\n    - Additive utilitarian (add all ratings) - DONE (edit: THIS IS JUST THE SAME AS AVERAGE!!!!)\n    - Plurality vote  (i don't know how to do this)\n- Sort for each Aggregation strategy (n lists)\n- cut list by k-numbers (top-k group recommendation) (n lists)\n- get Group satisfaction metric for each n-list","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"19fc60ca-8d61-45d7-8426-f556c2db13ac"},{"version":"CommandV1","origId":995,"guid":"677f05e9-e8f8-4792-a19d-327e62114752","subtype":"command","commandType":"auto","position":19.7421875,"command":"from __builtin__ import max\n\ndef sort_and_normalize(product_rating_pairs):\n  max_rating = max(map(lambda x: x[1], product_rating_pairs))\n#   return max_rating\n  product_rating_pairs_normalized = map(lambda x: (x[0], x[1]/max_rating) , product_rating_pairs)\n  sorted_normalized_pairs = sorted(product_rating_pairs_normalized, key=lambda y: y[1], reverse=True)\n  return sorted_normalized_pairs\n\n# predictions_keys = get_predictions_for_users(grp, model).map(lambda x: (x[0], (x[1], x[2]))) #putting map action into get_predictions_for _users function\n# predictions_by_user = predictions_keys.groupByKey().map(lambda x : (x[0], sort_and_normalize(x[1])))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;RDD&apos; object has no attribute &apos;groupBykey&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-105-c65d876eaff6&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>predictions<span class=\"ansiyellow\">.</span>groupBykey<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">(</span>p<span class=\"ansiyellow\">.</span>user<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>p<span class=\"ansiyellow\">.</span>product<span class=\"ansiyellow\">,</span> p<span class=\"ansiyellow\">.</span>rating<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansired\"># features = predictions.map(lambda p: p.rating)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansigreen\">def</span> sort_and_normalize<span class=\"ansiyellow\">(</span>product_rating_pairs<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span>   max <span class=\"ansiyellow\">=</span> max<span class=\"ansiyellow\">(</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> product_rating_pairs<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;RDD&apos; object has no attribute &apos;groupBykey&apos;\n</div>","workflows":[],"startTime":1.482028524567E12,"submitTime":1.482028523175E12,"finishTime":1.482028524638E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4d575892-7c98-4802-a0b3-b1f3a4d333f2"},{"version":"CommandV1","origId":963,"guid":"9e9763a1-c7ee-4248-9a43-98e8966a3616","subtype":"command","commandType":"auto","position":19.81982421875,"command":"# Function to create list of all NORMALIZED predictions for users in a grp:\n# This will output a key pair of [user, longlist of songs and normalized ratings] for n-users\n\ndef get_predictions_for_users(grp, model):\n  # Get all users in the group\n  users_ingroup = [u[0] for u in grp]\n  # Get all products in the model\n  products_list = model.productFeatures().map(lambda p: p[0]) # using saved model\n  # Create all possible user product pairs\n  users_ingroup_product_pairs = sc.parallelize(users_ingroup).cartesian(products_list)\n  # Get predictions for each user\n  predictions_keys = model.predictAll(users_ingroup_product_pairs).map(lambda x: (x[0], (x[1], x[2])))\n  predictions_by_user = predictions_keys.groupByKey().map(lambda x : (x[0], sort_and_normalize(x[1])))\n  return predictions_by_user\n\n\n# # Get product, rating key pair for all users\n# prod_rate = predictions.map(lambda p: (p.product, p.rating))","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;grp&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-4-403abcbc59e4&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span> <span class=\"ansired\"># Get users in group</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 5</span><span class=\"ansiyellow\"> </span>users_ingroup <span class=\"ansiyellow\">=</span> <span class=\"ansiyellow\">[</span>u<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span> <span class=\"ansigreen\">for</span> u <span class=\"ansigreen\">in</span> grp<span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      6</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      7</span> <span class=\"ansired\"># # Get the unique products</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;grp&apos; is not defined\n</div>","workflows":[],"startTime":1.482028526712E12,"submitTime":1.482028525316E12,"finishTime":1.482028526752E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"66353b36-fa0e-407f-9b28-b88295625f5f"},{"version":"CommandV1","origId":1042,"guid":"10289293-4141-4c13-ba57-b23f68b665d8","subtype":"command","commandType":"auto","position":19.86187744140625,"command":"grptest = [(44250, 313), (17250, 334), (49550, 971)]","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481913988733E12,"submitTime":1.481913987223E12,"finishTime":1.481913988809E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"627759f0-3719-4fdf-afe7-ebba2fc9665d"},{"version":"CommandV1","origId":1021,"guid":"077f7414-cf40-47b5-b079-17a9a2ff6de6","subtype":"command","commandType":"auto","position":19.9039306640625,"command":"# print ho_grp\n# type(model)\n\ntest = get_predictions_for_users(grptest, model)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: global name &apos;predictions&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-13-9c8ba8b43597&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansired\"># type(model)</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 4</span><span class=\"ansiyellow\"> </span>test <span class=\"ansiyellow\">=</span> get_predictions_for_users<span class=\"ansiyellow\">(</span>ho_grp<span class=\"ansiyellow\">,</span> model<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-11-9a22bd1adb76&gt;</span> in <span class=\"ansicyan\">get_predictions_for_users</span><span class=\"ansiblue\">(grp, model)</span>\n<span class=\"ansigreen\">     11</span>   predictions_keys <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>predictAll<span class=\"ansiyellow\">(</span>users_ingroup_product_pairs<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     12</span>   predictions_by_user <span class=\"ansiyellow\">=</span> predictions_keys<span class=\"ansiyellow\">.</span>groupByKey<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x <span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> sort_and_normalize<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 13</span><span class=\"ansiyellow\">   </span><span class=\"ansigreen\">return</span> predictions<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: global name &apos;predictions&apos; is not defined\n</div>","workflows":[],"startTime":1.481913993085E12,"submitTime":1.481913991531E12,"finishTime":1.481914023883E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1f0832ec-2b8a-462c-98de-410b0e66155a"},{"version":"CommandV1","origId":1004,"guid":"5f582ccb-04ac-494d-8b0b-58c3091b3423","subtype":"command","commandType":"auto","position":19.988037109375,"command":"# Least Misery Aggregation Strategy on Normalized Ratings!!!!\n# predictions_by_user.flatMapValues(lambda p: p).map(lambda x: Rating(x[0], x[1][0], x[1][1])).take(3)\n\ndef get_lm_recolist(grp, predictions_for_users, list_length):\n  filtergrp = [u[0] for u in grp]\n  lm_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(min).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return lm_recolist","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 125 in stage 2032.0 failed 4 times, most recent failure: Lost task 125.3 in stage 2032.0 (TID 39667, 10.172.241.8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-180-05d8e5cb9d7f&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>predictions_by_user<span class=\"ansiyellow\">.</span>flatMapValues<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> Rating<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">take</span><span class=\"ansiblue\">(self, num)</span>\n<span class=\"ansigreen\">   1308</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1309</span>             p <span class=\"ansiyellow\">=</span> range<span class=\"ansiyellow\">(</span>partsScanned<span class=\"ansiyellow\">,</span> min<span class=\"ansiyellow\">(</span>partsScanned <span class=\"ansiyellow\">+</span> numPartsToTry<span class=\"ansiyellow\">,</span> totalParts<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">-&gt; 1310</span><span class=\"ansiyellow\">             </span>res <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">,</span> takeUpToNumLeft<span class=\"ansiyellow\">,</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1311</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1312</span>             items <span class=\"ansiyellow\">+=</span> res<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.pyc</span> in <span class=\"ansicyan\">runJob</span><span class=\"ansiblue\">(self, rdd, partitionFunc, partitions, allowLocal)</span>\n<span class=\"ansigreen\">    931</span>         <span class=\"ansired\"># SparkContext#runJob.</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    932</span>         mappedRDD <span class=\"ansiyellow\">=</span> rdd<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>partitionFunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 933</span><span class=\"ansiyellow\">         </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>runJob<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jsc<span class=\"ansiyellow\">.</span>sc<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">,</span> partitions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    934</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> mappedRDD<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    935</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 125 in stage 2032.0 failed 4 times, most recent failure: Lost task 125.3 in stage 2032.0 (TID 39667, 10.172.241.8): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;&lt;ipython-input-180-05d8e5cb9d7f&gt;&quot;, line 1, in &lt;lambda&gt;\nTypeError: __new__() takes exactly 4 arguments (3 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor111.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;/databricks/spark/python/pyspark/rdd.py&quot;, line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File &quot;&lt;ipython-input-180-05d8e5cb9d7f&gt;&quot;, line 1, in &lt;lambda&gt;\nTypeError: __new__() takes exactly 4 arguments (3 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.481937177796E12,"submitTime":1.481937176141E12,"finishTime":1.481937177865E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d23dbf6f-70d1-481d-8316-ee3de7f397f2"},{"version":"CommandV1","origId":1026,"guid":"0f076521-73ca-46be-af96-60c697f2bc08","subtype":"command","commandType":"auto","position":20.13555908203125,"command":"### TEST CODE\nget_lm_recolist(ho_grp, test, 5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">27</span><span class=\"ansired\">]: </span>\n[(7016, 0.8236982551140664),\n (175470, 0.817310640380297),\n (86957, 0.8166065832709695),\n (293104, 0.8108236713479806),\n (280887, 0.7722525362641703)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">AttributeError</span>: &apos;list&apos; object has no attribute &apos;collect&apos;","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-26-8cef225a5c36&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>get_lm_recolist<span class=\"ansiyellow\">(</span>ho_grp<span class=\"ansiyellow\">,</span> test<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AttributeError</span>: &apos;list&apos; object has no attribute &apos;collect&apos;\n</div>","workflows":[],"startTime":1.481904502233E12,"submitTime":1.481904500943E12,"finishTime":1.481904507367E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6a9045b8-ef67-446a-9c95-fdecfc04c0d2"},{"version":"CommandV1","origId":1006,"guid":"3b1bca2a-bbbf-47a9-a845-44d208cde852","subtype":"command","commandType":"auto","position":20.2830810546875,"command":"# Most Pleasure Aggregation Strategy on Normalized Ratings!!!\n\ndef get_mp_recolist(grp, predictions_for_users, list_length):\n  filtergrp = [u[0] for u in grp]\n  mp_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(max).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return mp_recolist","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481932229007E12,"submitTime":1.481932227547E12,"finishTime":1.481932229076E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0bff5010-0e7b-44a2-a65e-b8aaa92ea268"},{"version":"CommandV1","origId":1027,"guid":"de854b72-16ec-40d0-8379-a166415e1301","subtype":"command","commandType":"auto","position":20.43060302734375,"command":"### TEST CODE\nget_mp_recolist(ho_grp, test, 5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">30</span><span class=\"ansired\">]: </span>\n[(276684, 1.0),\n (86957, 1.0),\n (350908, 1.0),\n (186927, 0.9922395990013464),\n (136472, 0.9819729804494817)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;get_mp_recolist&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-28-4cc435a76b46&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>get_mp_recolist<span class=\"ansiyellow\">(</span>ho_grp<span class=\"ansiyellow\">,</span> test<span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;get_mp_recolist&apos; is not defined\n</div>","workflows":[],"startTime":1.481904537929E12,"submitTime":1.481904536635E12,"finishTime":1.481904543062E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"6a0ff4a7-c469-411b-ba0a-9c0c8496c5df"},{"version":"CommandV1","origId":965,"guid":"b322930e-fcc1-4e6c-aebb-7b76a88ef94c","subtype":"command","commandType":"auto","position":20.578125,"command":"# Average Aggregation Strategy on Normalized Ratings!!!\n\ndef get_ave_recolist____\nsumCount = predictions_by_user.flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])).combineByKey(lambda value: (value, 1),\n                             lambda x, value: (x[0] + value, x[1] + 1),\n                             lambda x, y: (x[0] + y[0], x[1] + y[1]))\n\naverage_agg = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count))\n\nprint average_agg.sortBy(lambda p: p[1], ascending = False).take(10)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">[(86957, 0.9332640618875575), (7016, 0.8688779862045757), (175470, 0.8628373357485497), (293104, 0.8365906027280158), (46840, 0.8204148186213858), (276684, 0.8180150141807782), (136472, 0.8101503695892083), (243846, 0.808807786592197), (326792, 0.8068298080868411), (280887, 0.7778747167005916)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;prod_rate&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-62-729b6894a303&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansired\"># Average Aggregation Strategy</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> sumCount = prod_rate.combineByKey(lambda value: (value, 1),\n</span><span class=\"ansigreen\">      3</span>                              <span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">,</span> value<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">+</span> value<span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span> <span class=\"ansiyellow\">+</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      4</span>                              lambda x, y: (x[0] + y[0], x[1] + y[1]))\n<span class=\"ansigreen\">      5</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;prod_rate&apos; is not defined\n</div>","workflows":[],"startTime":1.481852131232E12,"submitTime":1.481852130366E12,"finishTime":1.481852140268E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9195589f-8cf6-44af-9e1d-b64cd821bd0d"},{"version":"CommandV1","origId":1008,"guid":"f0a79779-e2f9-451d-a6f3-a252b04193b2","subtype":"command","commandType":"auto","position":22.2890625,"command":"## CAN THIS BE USED IN PLACE OV AVERAGE??? BECAUSE SORTING LOOKS THE SAME\n\nfrom operator import add\ndef get_ave_recolist(grp, predictions_for_users, list_length):\n  filtergrp = [u[0] for u in grp]\n  ave_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(add).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return ave_recolist","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;add&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-186-8d81d56af8f0&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>predictions_by_user<span class=\"ansiyellow\">.</span>flatMapValues<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>reduceByKey<span class=\"ansiyellow\">(</span>add<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sortBy<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> ascending <span class=\"ansiyellow\">=</span> False<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;add&apos; is not defined\n</div>","workflows":[],"startTime":1.481982733564E12,"submitTime":1.481982730559E12,"finishTime":1.481982733603E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"196d3a45-bd4f-407c-9b84-2b7debb905e5"},{"version":"CommandV1","origId":1028,"guid":"29d80388-a75a-4193-9623-086073be1d06","subtype":"command","commandType":"auto","position":23.14453125,"command":"### TEST CODE\nget_ave_recolist(ho_grp, test, 5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">33</span><span class=\"ansired\">]: </span>\n[(86957, 2.7997921856626724),\n (7016, 2.606633958613727),\n (175470, 2.588512007245649),\n (293104, 2.509771808184048),\n (46840, 2.4612444558641573)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481904588988E12,"submitTime":1.481904587692E12,"finishTime":1.481904593617E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"236928a0-c3b2-4eca-8fbe-07ad48b947a7"},{"version":"CommandV1","origId":1053,"guid":"ee8bc4fa-96f3-47eb-acbc-3db8ca0c84ce","subtype":"command","commandType":"auto","position":23.572265625,"command":"# Multiply Aggregation Strategy on Normalized Ratings!!!\nfrom operator import mul\ndef get_mul_recolist(grp, predictions_for_users, list_length):\n  filtergrp = [u[0] for u in grp]\n  ave_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(mul).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return ave_recolist","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481982731032E12,"submitTime":1.481982728018E12,"finishTime":1.481982731104E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1e6aa627-109d-4a11-8de1-58571a518ad4"},{"version":"CommandV1","origId":968,"guid":"16c89408-7144-47bf-bf80-b6751356529e","subtype":"command","commandType":"auto","position":24.0,"command":"%md # Statistical Analysis: Calculating best Aggregation Strategy by Group Satisfaction  \n### This will be used for the analysis part only.","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"fcc47873-4b0b-4138-9263-43135413a1b2"},{"version":"CommandV1","origId":969,"guid":"c92558ef-1c79-4fbd-bfb4-2fa561c53ddf","subtype":"command","commandType":"auto","position":25.0,"command":"%md With the playlist order given, get the user satisfaction by getting the (sum(rating)/#songs)","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4f464747-e262-4bf9-a1be-6e3a72ee5df7"},{"version":"CommandV1","origId":1145,"guid":"134767e5-d0cb-4940-8fe6-231f5d4542bf","subtype":"command","commandType":"auto","position":25.5,"command":"\"\"\"\n# Before running the function:\n# - MUST GET ALL USERS in test groupings to be used below - DONE\n# - MUST GET predictions_for_users for all users in the test groupings: will be named \"predictions\" - DONE\n\n# List all groupings here. Copy from previously created groupings.\nho_grps_3 = [[(126650, 741), (174250, 741), (383650, 741)], [(84250, 722), (92650, 722), (193650, 722)] , [(7550, 565), (178350, 565), (266450, 565)], [(322750, 814), (368450, 814), (461150, 814)], [(87350, 188), (463450, 188), (570450, 188)], [(209050, 27), (229850, 27), (393150, 27)], [(233950, 308), (327250, 308), (338950, 308)], [(146950, 550), (224750, 550), (264350, 550)], [(155650, 189), (240250, 189), (559250, 189)], [(78150, 601), (164950, 601), (207650, 601)], [(16450, 226), (259450, 226), (761250, 226)], [(48850, 957), (332250, 957), (513950, 957)], [(31650, 818), (57350, 818), (93450, 818)], [(186950, 289), (292150, 289), (840750, 289)], [(171750, 208), (278950, 208), (294450, 208)]]\nhe_grps_3 = [[(44250, 313), (17250, 334), (49550, 971)], [(1008450, 714), (64950, 362), (2150, 82)], [(34150, 194), (90550, 291), (231650, 422)], [(29050, 441), (64450, 929), (110150, 941)], [(347250, 592), (249250, 414), (110650, 572)], [(17450, 899), (9950, 981), (37250, 436)], [(146950, 550), (139450, 328), (1850, 89)], [(50050, 41), (157150, 588), (1450, 802)], [(767950, 674), (169950, 251), (121750, 530)], [(224050, 522), (151350, 649), (225450, 272)], [(250950, 830), (35550, 292), (117050, 909)], [(55350, 580), (63650, 329), (19850, 449)], [(191350, 541), (63850, 371), (16450, 226)], [(245350, 859), (187250, 280)], [(168650, 570), (103850, 930), (11250, 60)]]\nho_grps_5 = [[(2850, 627), (7450, 627), (60050, 627), (113650, 627), (115750, 627)], [(47150, 290), (89650, 290), (91250, 290), (114150, 290), (173450, 290)], [(16350, 106), (82750, 106), (91550, 106), (141250, 106), (148450, 106)], [(176050, 296), (613750, 296), (22560, 296), (283360, 296), (339060, 296)], [(16150, 418), (88350, 418), (179150, 418), (271750, 418), (311550, 418)], [(150950, 840), (538950, 840), (617050, 840), (731050, 840), (799850, 840)], [(44450, 552), (47850, 552), (134850, 552), (157450, 552), (185750, 552)], [(11950, 467), (119350, 467), (190650, 467), (231750, 467), (282850, 467)], [(103750, 343), (124350, 343), (181150, 343), (189450, 343), (207150, 343)], [(136050, 842), (329250, 842), (525550, 842), (719550, 842), (746850, 842)], [(345350, 497), (404150, 497), (681850, 497), (33260, 497), (89560, 497)], [(250150, 848), (111060, 848), (155360, 848), (385360, 848), (481264, 848)], [(293150, 681), (402050, 681), (578850, 681), (583750, 681), (650550, 681)], [(49950, 544), (50550, 544), (192650, 544), (220250, 544), (235850, 544)], [(13050, 330), (68150, 330), (124950, 330), (148050, 330), (160350, 330)]]\nhe_grps_5 = [[(629850, 396), (194150, 182), (34150, 194), (19950, 406), (494860, 911)], [(219050, 515), (63550, 931), (64850, 789), (187550, 743), (315850, 491)], [(4050, 52), (450, 903), (32650, 968), (9950, 981), (319850, 826)], [(40650, 417), (181450, 389), (453960, 388), (76150, 77), (10550, 471)], [(12650, 811), (181650, 586), (3550, 488), (37550, 594), (16650, 29)], [(351450, 509), (1750, 462), (168550, 711), (150950, 840), (58750, 958)], [(17050, 628), (82050, 694), (24750, 169), (149050, 455), (70750, 915)], [(48150, 369), (47450, 720), (26750, 961), (13750, 548), (41350, 103)], [(37050, 705), (35960, 16), (116650, 965), (172750, 963), (494550, 149)], [(17450, 899), (63650, 329), (89050, 439), (19950, 406), (48050, 829)], [(353550, 203), (50650, 742), (238250, 35), (29050, 441), (67950, 535)], [(110650, 572), (84050, 318), (57850, 242), (29850, 872), (47050, 43)], [(5350, 900), (71250, 871), (50050, 41), (28450, 608)], [(53450, 271), (45750, 20), (4550, 730), (128750, 817), (120750, 454)], [(336950, 457), (74050, 687), (93650, 972), (12550, 804), (214350, 97)]]\nho_grps_7 = [[(209050, 27), (229850, 27), (393150, 27), (404450, 27), (720050, 27), (779950, 27), (916750, 27)], [(44350, 373), (252450, 373), (268850, 373), (279150, 373), (840850, 373), (854050, 373), (17460, 373)], [(1008750, 865), (458960, 865), (580960, 865), (885260, 865), (208564, 865), (908264, 865), (106766, 865)], [(28550, 119), (54350, 119), (79250, 119), (124150, 119), (156050, 119), (326650, 119), (330150, 119)], [(849350, 591), (925950, 591), (1057550, 591), (641360, 591), (685760, 591), (884560, 591), (138664, 591)], [(453250, 740), (485850, 740), (542150, 740), (576850, 740), (619650, 740), (749950, 740), (828350, 740)], [(29050, 441), (103550, 441), (130350, 441), (148350, 441), (174550, 441), (185350, 441), (205850, 441)], [(148650, 385), (226750, 385), (247850, 385), (297750, 385), (447650, 385), (459850, 385), (670050, 385)], [(71250, 871), (141750, 871), (165150, 871), (242150, 871), (420950, 871), (435650, 871), (543250, 871)], [(182150, 364), (167360, 364), (250960, 364), (255360, 364), (295560, 364), (406260, 364), (557260, 364)], [(4450, 688), (64250, 688), (139550, 688), (147250, 688), (207950, 688), (319250, 688), (364850, 688)], [(680750, 322), (749550, 322), (750950, 322), (256860, 322), (617960, 322), (680060, 322), (814360, 322)], [(56550, 636), (269050, 636), (531650, 636), (672150, 636), (950750, 636), (17160, 636), (172860, 636)], [(20550, 56), (30850, 56), (80450, 56), (135450, 56), (167650, 56), (199750, 56), (234750, 56)], [(34350, 792), (577150, 792), (61360, 792), (72660, 792), (84960, 792), (98760, 792), (236760, 792)]]\nhe_grps_7 = [[(163950, 787), (168650, 570), (319650, 749), (48150, 369), (163550, 519), (8350, 277), (40250, 276)], [(680750, 322), (41750, 719), (27150, 380), (67350, 690), (192550, 849), (26750, 961), (48250, 868)], [(833350, 866), (59850, 755), (102650, 6), (45950, 447), (191550, 109), (465250, 656), (547250, 156)], [(28150, 770), (402450, 982), (42050, 584), (402150, 425), (242350, 429), (58650, 345), (38950, 170)], [(143850, 493), (279650, 846), (42150, 660), (1150, 980), (42750, 326), (273550, 423), (6050, 382)], [(51050, 614), (232650, 50), (767950, 674), (289464, 761), (186950, 289), (196150, 435), (54850, 798)], [(140550, 664), (8650, 734), (41750, 719), (213199, 933), (136250, 154), (118150, 254)], [(38850, 282), (37750, 567), (2950, 288), (245350, 859), (345350, 497), (10350, 881), (85050, 145)], [(250150, 848), (6050, 382), (198650, 316), (24250, 723), (17450, 899), (12350, 18), (187450, 732)], [(5450, 663), (80350, 885), (129050, 230), (55250, 945), (38850, 282), (21450, 934), (68850, 198)], [(16350, 106), (30950, 805), (150550, 285), (32850, 659), (11950, 467), (46550, 499), (55150, 440)], [(185650, 1), (123550, 547), (157150, 588), (95450, 267), (114950, 211), (168850, 427), (10450, 918)], [(125450, 3), (48150, 369), (9350, 773), (34050, 99), (28550, 119), (82050, 694), (22750, 332)], [(482250, 752), (26550, 73), (157350, 53), (95450, 267), (79750, 862), (354950, 901), (3550, 488)], [(34350, 792), (236650, 482), (263450, 667), (50050, 41), (849450, 767), (833350, 866), (114950, 211)]]\n\n# END\n\"\"\"","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"193b2600-48a5-44ee-b293-fa47e3e40558"},{"version":"CommandV1","origId":971,"guid":"3590248e-7a82-4182-ae2c-1042d167940f","subtype":"command","commandType":"auto","position":26.0,"command":"# Set playlist length\nlist_length = 20\n\n# Getting group satisfaction\nimport numpy as np\nfrom operator import add\nfrom operator import mul\ndef get_stat_grp_sat(bulk_grp, bulk_preds):\n  \"\"\"\n  Input: bulk groupings: example. ho_grp_3 is 15 groups with 3 members each from the same cluster.\n  Output: Mean satisfaction score for all samples per aggregation strategy.\n  \"\"\"\n  all_sat = []\n#   print all_sat\n  for grp in bulk_grp:\n    filtergrp = [u[0] for u in grp]\n    filteredpreds = bulk_preds.filter(lambda p: p[0] in filtergrp)\n    filteredpreds.cache()\n    # Get recolist for each strategy:\n    # Least Misery\n    lm_recolist = filteredpreds \\\n      .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n      .reduceByKey(min).sortBy(lambda p: p[1], ascending = False).take(list_length)\n      \n#     print lm_recolist\n    lm_grpsat = get_grp_sat(grp, lm_recolist, filteredpreds)\n    \n    # Most Pleasure\n    mp_recolist = filteredpreds \\\n      .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n      .reduceByKey(max).sortBy(lambda p: p[1], ascending = False).take(list_length)\n      \n#     print mp_recolist\n    mp_grpsat = get_grp_sat(grp, mp_recolist, filteredpreds)\n    \n    # Average\n    av_recolist = filteredpreds \\\n      .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n      .reduceByKey(add).sortBy(lambda p: p[1], ascending = False).take(list_length)\n      \n#     print av_recolist\n    av_grpsat = get_grp_sat(grp, av_recolist, filteredpreds)\n    \n#     # Multiply\n#     mul_recolist = filteredpreds \\\n#       .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n#       .reduceByKey(mul).sortBy(lambda p: p[1], ascending = False).take(list_length)\n      \n#     mul_grpsat = get_grp_sat(grp, mul_recolist, filteredpreds)\n    \n#     all_sat.append((grp, lm_grpsat, mp_grpsat, av_grpsat, mul_grpsat))\n#   return all_sat\n\n\ndef get_grp_sat(group_to_get, reco_list, fpred):\n  songs_in_list = [s[0] for s in reco_list]\n  user_sat = []\n  for u in group_to_get:\n    user_sat.append(fpred.flatMapValues(lambda p: p).filter(lambda p: p[0] == u).map(lambda x: (x[1][0], x[1][1])).filter(lambda p: p[0] in songs_in_list).map(lambda p: p[1]).mean())\n  grp_sat = np.mean(user_sat)\n  return grp_sat","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.482029215061E12,"submitTime":1.482029190628E12,"finishTime":1.482029215147E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8ca8a93a-df6a-465d-8d53-6683f4f2f451"},{"version":"CommandV1","origId":1190,"guid":"f1f263eb-83ad-49d9-9f41-f73ebb2a4f50","subtype":"command","commandType":"auto","position":26.5,"command":"# Create consolidated group\n# cons_g = ho_grps_3 + he_grps_3 + ho_grps_5 + he_grps_5 + ho_grps_7 + he_grps_7\nsmall_g = ho_grps_3\n# END\n\n# Create list of all users in consolidated group\nflatten = lambda l: [item for sublist in l for item in sublist]\nall_users = flatten(small_g)\n# END\n\n\n# Get all predictions for all users (predictions_for_users) in the consolidated group\npredictions = get_predictions_for_users(all_users, model)\n# Command took 2.34 minutes\n\ntest = get_stat_grp_sat(small_g, predictions)\nprint test","commandVersion":0,"state":"error","results":null,"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1309.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1309.0 (TID 55870, 10.172.225.31): java.io.IOException: No space left on device","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-23-a8d0fc4e6ed0&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansired\"># Command took 2.34 minutes</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 16</span><span class=\"ansiyellow\"> </span>test <span class=\"ansiyellow\">=</span> get_stat_grp_sat<span class=\"ansiyellow\">(</span>small_g<span class=\"ansiyellow\">,</span> predictions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     17</span> <span class=\"ansigreen\">print</span> test<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-22-8daf45bf3b99&gt;</span> in <span class=\"ansicyan\">get_stat_grp_sat</span><span class=\"ansiblue\">(bulk_grp, bulk_preds)</span>\n<span class=\"ansigreen\">     19</span>     <span class=\"ansired\"># Get recolist for each strategy:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     20</span>     <span class=\"ansired\"># Least Misery</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 21</span><span class=\"ansiyellow\">     </span>lm_recolist <span class=\"ansiyellow\">=</span> filteredpreds       <span class=\"ansiyellow\">.</span>flatMapValues<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>       <span class=\"ansiyellow\">.</span>reduceByKey<span class=\"ansiyellow\">(</span>min<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sortBy<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> ascending <span class=\"ansiyellow\">=</span> False<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span>list_length<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     22</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     23</span> <span class=\"ansired\">#     print lm_recolist</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sortBy</span><span class=\"ansiblue\">(self, keyfunc, ascending, numPartitions)</span>\n<span class=\"ansigreen\">    660</span>         <span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;a&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;b&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;1&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;d&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">4</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;2&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    661</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 662</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>keyBy<span class=\"ansiyellow\">(</span>keyfunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sortByKey<span class=\"ansiyellow\">(</span>ascending<span class=\"ansiyellow\">,</span> numPartitions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>values<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    663</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    664</span>     <span class=\"ansigreen\">def</span> glom<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sortByKey</span><span class=\"ansiblue\">(self, ascending, numPartitions, keyfunc)</span>\n<span class=\"ansigreen\">    628</span>         <span class=\"ansired\"># the key-space into bins such that the bins have roughly the same</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    629</span>         <span class=\"ansired\"># number of (key, value) pairs falling into them</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 630</span><span class=\"ansiyellow\">         </span>rddSize <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>count<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    631</span>         <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> rddSize<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    632</span>             <span class=\"ansigreen\">return</span> self  <span class=\"ansired\"># empty RDD</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">count</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   1006</span>         <span class=\"ansicyan\">3</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1007</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1008</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> i<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span> <span class=\"ansigreen\">for</span> _ <span class=\"ansigreen\">in</span> i<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1009</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1010</span>     <span class=\"ansigreen\">def</span> stats<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sum</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    997</span>         <span class=\"ansicyan\">6.0</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    998</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 999</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>fold<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span> operator<span class=\"ansiyellow\">.</span>add<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1000</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1001</span>     <span class=\"ansigreen\">def</span> count<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">fold</span><span class=\"ansiblue\">(self, zeroValue, op)</span>\n<span class=\"ansigreen\">    871</span>         <span class=\"ansired\"># zeroValue provided to each partition is unique from the one provided</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    872</span>         <span class=\"ansired\"># to the final reduce call</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 873</span><span class=\"ansiyellow\">         </span>vals <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    874</span>         <span class=\"ansigreen\">return</span> reduce<span class=\"ansiyellow\">(</span>op<span class=\"ansiyellow\">,</span> vals<span class=\"ansiyellow\">,</span> zeroValue<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    875</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    774</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 776</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    777</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1309.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1309.0 (TID 55870, 10.172.225.31): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver$$anonfun$writeIndexFileAndCommit$2.apply$mcV$sp(IndexShuffleBlockResolver.scala:153)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1319)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:152)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\tSuppressed: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t... 11 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver$$anonfun$writeIndexFileAndCommit$2.apply$mcV$sp(IndexShuffleBlockResolver.scala:153)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1319)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:152)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\tSuppressed: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t... 11 more\n\n</div>","workflows":[],"startTime":1.48202922596E12,"submitTime":1.48202922596E12,"finishTime":1.482029248847E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"8aedb4f4-51e2-4aa8-996c-a46b9e8ac6fa"},{"version":"CommandV1","origId":1191,"guid":"2fa7b220-a0e3-46e9-bfe7-d6d198b759a4","subtype":"command","commandType":"auto","position":26.75,"command":"# Create consolidated group\n# cons_g = ho_grps_3 + he_grps_3 + ho_grps_5 + he_grps_5 + ho_grps_7 + he_grps_7\nsmall_g = he_grps_3\n# END\n\n# Create list of all users in consolidated group\nflatten = lambda l: [item for sublist in l for item in sublist]\nall_users = flatten(small_g)\n# END\n\n\n# Get all predictions for all users (predictions_for_users) in the consolidated group\npredictions = get_predictions_for_users(all_users, model)\n# Command took 2.34 minutes\n\ntest = get_stat_grp_sat(small_g, predictions)\nprint test","commandVersion":0,"state":"error","results":null,"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1258.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1258.0 (TID 54480, 10.172.225.32): java.io.IOException: No space left on device","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-17-c3ea4325aae4&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansired\"># Command took 2.34 minutes</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 16</span><span class=\"ansiyellow\"> </span>test <span class=\"ansiyellow\">=</span> get_stat_grp_sat<span class=\"ansiyellow\">(</span>small_g<span class=\"ansiyellow\">,</span> predictions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     17</span> <span class=\"ansigreen\">print</span> test<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-15-05c50704a49f&gt;</span> in <span class=\"ansicyan\">get_stat_grp_sat</span><span class=\"ansiblue\">(bulk_grp, bulk_preds)</span>\n<span class=\"ansigreen\">     19</span>     <span class=\"ansired\"># Get recolist for each strategy:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     20</span>     <span class=\"ansired\"># Least Misery</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 21</span><span class=\"ansiyellow\">     </span>lm_recolist <span class=\"ansiyellow\">=</span> filteredpreds       <span class=\"ansiyellow\">.</span>flatMapValues<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>       <span class=\"ansiyellow\">.</span>reduceByKey<span class=\"ansiyellow\">(</span>min<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sortBy<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> ascending <span class=\"ansiyellow\">=</span> False<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span>list_length<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     22</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     23</span> <span class=\"ansired\">#     print lm_recolist</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sortBy</span><span class=\"ansiblue\">(self, keyfunc, ascending, numPartitions)</span>\n<span class=\"ansigreen\">    660</span>         <span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;a&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;b&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;1&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;d&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">4</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;2&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    661</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 662</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>keyBy<span class=\"ansiyellow\">(</span>keyfunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sortByKey<span class=\"ansiyellow\">(</span>ascending<span class=\"ansiyellow\">,</span> numPartitions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>values<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    663</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    664</span>     <span class=\"ansigreen\">def</span> glom<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sortByKey</span><span class=\"ansiblue\">(self, ascending, numPartitions, keyfunc)</span>\n<span class=\"ansigreen\">    628</span>         <span class=\"ansired\"># the key-space into bins such that the bins have roughly the same</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    629</span>         <span class=\"ansired\"># number of (key, value) pairs falling into them</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 630</span><span class=\"ansiyellow\">         </span>rddSize <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>count<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    631</span>         <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> rddSize<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    632</span>             <span class=\"ansigreen\">return</span> self  <span class=\"ansired\"># empty RDD</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">count</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   1006</span>         <span class=\"ansicyan\">3</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1007</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1008</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> i<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span> <span class=\"ansigreen\">for</span> _ <span class=\"ansigreen\">in</span> i<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1009</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1010</span>     <span class=\"ansigreen\">def</span> stats<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sum</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    997</span>         <span class=\"ansicyan\">6.0</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    998</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 999</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>fold<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span> operator<span class=\"ansiyellow\">.</span>add<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1000</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1001</span>     <span class=\"ansigreen\">def</span> count<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">fold</span><span class=\"ansiblue\">(self, zeroValue, op)</span>\n<span class=\"ansigreen\">    871</span>         <span class=\"ansired\"># zeroValue provided to each partition is unique from the one provided</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    872</span>         <span class=\"ansired\"># to the final reduce call</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 873</span><span class=\"ansiyellow\">         </span>vals <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    874</span>         <span class=\"ansigreen\">return</span> reduce<span class=\"ansiyellow\">(</span>op<span class=\"ansiyellow\">,</span> vals<span class=\"ansiyellow\">,</span> zeroValue<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    875</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    774</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 776</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    777</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1258.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1258.0 (TID 54480, 10.172.225.32): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver$$anonfun$writeIndexFileAndCommit$2.apply$mcV$sp(IndexShuffleBlockResolver.scala:153)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1319)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:152)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\tSuppressed: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t... 11 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver$$anonfun$writeIndexFileAndCommit$2.apply$mcV$sp(IndexShuffleBlockResolver.scala:153)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1319)\n\tat org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:152)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\tSuppressed: java.io.IOException: No space left on device\n\t\tat java.io.FileOutputStream.writeBytes(Native Method)\n\t\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\t\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\t\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:158)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\t... 11 more\n\n</div>","workflows":[],"startTime":1.482028699512E12,"submitTime":1.482028699512E12,"finishTime":1.48202908281E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9b40524e-8a54-453d-ae1d-6215ec0c4348"},{"version":"CommandV1","origId":1038,"guid":"bd6ab639-afb1-4e19-8d1b-c57e0907bc6f","subtype":"command","commandType":"auto","position":27.0,"command":"# Create consolidated group\n# cons_g = ho_grps_3 + he_grps_3 + ho_grps_5 + he_grps_5 + ho_grps_7 + he_grps_7\nsmall_g = ho_grps_5\n# END\n\n# Create list of all users in consolidated group\nflatten = lambda l: [item for sublist in l for item in sublist]\nall_users = flatten(small_g)\n# END\n\n\n# Get all predictions for all users (predictions_for_users) in the consolidated group\npredictions = get_predictions_for_users(all_users, model)\n# Command took 2.34 minutes\n\ntest = get_stat_grp_sat(small_g, predictions)\nprint test","commandVersion":0,"state":"error","results":null,"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 1285.0 failed 4 times, most recent failure: Lost task 23.3 in stage 1285.0 (TID 54940, 10.172.225.31): java.io.IOException: No space left on device","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-18-d09af6ff6b52&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">     14</span> <span class=\"ansired\"># Command took 2.34 minutes</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     15</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 16</span><span class=\"ansiyellow\"> </span>test <span class=\"ansiyellow\">=</span> get_stat_grp_sat<span class=\"ansiyellow\">(</span>small_g<span class=\"ansiyellow\">,</span> predictions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     17</span> <span class=\"ansigreen\">print</span> test<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-15-05c50704a49f&gt;</span> in <span class=\"ansicyan\">get_stat_grp_sat</span><span class=\"ansiblue\">(bulk_grp, bulk_preds)</span>\n<span class=\"ansigreen\">     19</span>     <span class=\"ansired\"># Get recolist for each strategy:</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     20</span>     <span class=\"ansired\"># Least Misery</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 21</span><span class=\"ansiyellow\">     </span>lm_recolist <span class=\"ansiyellow\">=</span> filteredpreds       <span class=\"ansiyellow\">.</span>flatMapValues<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>       <span class=\"ansiyellow\">.</span>reduceByKey<span class=\"ansiyellow\">(</span>min<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sortBy<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> ascending <span class=\"ansiyellow\">=</span> False<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>take<span class=\"ansiyellow\">(</span>list_length<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     22</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     23</span> <span class=\"ansired\">#     print lm_recolist</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sortBy</span><span class=\"ansiblue\">(self, keyfunc, ascending, numPartitions)</span>\n<span class=\"ansigreen\">    660</span>         <span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;a&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;b&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">2</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;1&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">3</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;d&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">4</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;2&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">5</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    661</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 662</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>keyBy<span class=\"ansiyellow\">(</span>keyfunc<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sortByKey<span class=\"ansiyellow\">(</span>ascending<span class=\"ansiyellow\">,</span> numPartitions<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>values<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    663</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    664</span>     <span class=\"ansigreen\">def</span> glom<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sortByKey</span><span class=\"ansiblue\">(self, ascending, numPartitions, keyfunc)</span>\n<span class=\"ansigreen\">    628</span>         <span class=\"ansired\"># the key-space into bins such that the bins have roughly the same</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    629</span>         <span class=\"ansired\"># number of (key, value) pairs falling into them</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 630</span><span class=\"ansiyellow\">         </span>rddSize <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>count<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    631</span>         <span class=\"ansigreen\">if</span> <span class=\"ansigreen\">not</span> rddSize<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    632</span>             <span class=\"ansigreen\">return</span> self  <span class=\"ansired\"># empty RDD</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">count</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">   1006</span>         <span class=\"ansicyan\">3</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1007</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1008</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> i<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span> <span class=\"ansigreen\">for</span> _ <span class=\"ansigreen\">in</span> i<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>sum<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1009</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1010</span>     <span class=\"ansigreen\">def</span> stats<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">sum</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    997</span>         <span class=\"ansicyan\">6.0</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    998</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 999</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">[</span>sum<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>fold<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">,</span> operator<span class=\"ansiyellow\">.</span>add<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1000</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1001</span>     <span class=\"ansigreen\">def</span> count<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">fold</span><span class=\"ansiblue\">(self, zeroValue, op)</span>\n<span class=\"ansigreen\">    871</span>         <span class=\"ansired\"># zeroValue provided to each partition is unique from the one provided</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    872</span>         <span class=\"ansired\"># to the final reduce call</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 873</span><span class=\"ansiyellow\">         </span>vals <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>mapPartitions<span class=\"ansiyellow\">(</span>func<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    874</span>         <span class=\"ansigreen\">return</span> reduce<span class=\"ansiyellow\">(</span>op<span class=\"ansiyellow\">,</span> vals<span class=\"ansiyellow\">,</span> zeroValue<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    875</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    774</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 776</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    777</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 1285.0 failed 4 times, most recent failure: Lost task 23.3 in stage 1285.0 (TID 54940, 10.172.225.31): java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:225)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1823)\n\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719)\n\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:56)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n\tat org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:714)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:72)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.IOException: No space left on device\n\tat java.io.FileOutputStream.writeBytes(Native Method)\n\tat java.io.FileOutputStream.write(FileOutputStream.java:326)\n\tat org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)\n\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n\tat net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:225)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.flush(ObjectOutputStream.java:1823)\n\tat java.io.ObjectOutputStream.flush(ObjectOutputStream.java:719)\n\tat org.apache.spark.serializer.JavaSerializationStream.flush(JavaSerializer.scala:56)\n\tat org.apache.spark.storage.DiskBlockObjectWriter.commitAndClose(DiskBlockObjectWriter.scala:130)\n\tat org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:714)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:72)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.482028699974E12,"submitTime":1.482028699974E12,"finishTime":1.4820291465E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"b6016e9b-f700-4e8f-9ebf-193edde64fca"},{"version":"CommandV1","origId":1184,"guid":"970ab76a-7af1-48c5-b348-25fd45ac17ea","subtype":"command","commandType":"auto","position":27.1875,"command":"# Create consolidated group\n# cons_g = ho_grps_3 + he_grps_3 + ho_grps_5 + he_grps_5 + ho_grps_7 + he_grps_7\nsmall_g = he_grps_5\n# END\n\n# Create list of all users in consolidated group\nflatten = lambda l: [item for sublist in l for item in sublist]\nall_users = flatten(small_g)\n# END\n\n\n# Get all predictions for all users (predictions_for_users) in the consolidated group\npredictions = get_predictions_for_users(all_users, model)\n# Command took 2.34 minutes\n\ntest = get_stat_grp_sat(small_g, predictions)\nprint test","commandVersion":0,"state":"error","results":null,"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.482028700538E12,"submitTime":1.482028700538E12,"finishTime":1.482029181766E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"efb901de-7e01-46b5-a4cb-84ffdb45613d"},{"version":"CommandV1","origId":1185,"guid":"3b51711a-b82c-40a9-89d1-529dd373aff2","subtype":"command","commandType":"auto","position":27.28125,"command":"# Create consolidated group\n# cons_g = ho_grps_3 + he_grps_3 + ho_grps_5 + he_grps_5 + ho_grps_7 + he_grps_7\nsmall_g = ho_grps_7\n# END\n\n# Create list of all users in consolidated group\nflatten = lambda l: [item for sublist in l for item in sublist]\nall_users = flatten(small_g)\n# END\n\n\n# Get all predictions for all users (predictions_for_users) in the consolidated group\npredictions = get_predictions_for_users(all_users, model)\n# Command took 2.34 minutes\n\ntest = get_stat_grp_sat(small_g, predictions)\nprint test","commandVersion":0,"state":"error","results":null,"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.482028701066E12,"submitTime":1.482028701066E12,"finishTime":1.482029184845E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"72fa982e-f2b1-4bc3-8e57-77c42ae57481"},{"version":"CommandV1","origId":1186,"guid":"f1ce44ed-1452-4095-96e8-91416bd1596a","subtype":"command","commandType":"auto","position":27.328125,"command":"# Create consolidated group\n# cons_g = ho_grps_3 + he_grps_3 + ho_grps_5 + he_grps_5 + ho_grps_7 + he_grps_7\nsmall_g = he_grps_7\n# END\n\n# Create list of all users in consolidated group\nflatten = lambda l: [item for sublist in l for item in sublist]\nall_users = flatten(small_g)\n# END\n\n\n# Get all predictions for all users (predictions_for_users) in the consolidated group\npredictions = get_predictions_for_users(all_users, model)\n# Command took 2.34 minutes\n\ntest = get_stat_grp_sat(small_g, predictions)\nprint test","commandVersion":0,"state":"error","results":null,"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.482028701709E12,"submitTime":1.482028701709E12,"finishTime":1.482029216918E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"ff13d6c9-61bc-423a-bfd6-888ed0b51db4"},{"version":"CommandV1","origId":1148,"guid":"fa4d6a36-f922-4155-841f-9fce24c74969","subtype":"command","commandType":"auto","position":27.375,"command":"# ho_grps_3 + he_grps_3 + ho_grps_5 + he_grps_5 + ho_grps_7 + he_grps_7\n# small_g = ho_grps_3","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481948364298E12,"submitTime":1.481948362293E12,"finishTime":1.48194836432E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9c29d319-faa3-40c3-bdde-e1ec2a01b14e"},{"version":"CommandV1","origId":1040,"guid":"5956e82f-7b6e-48c6-8d7b-0cfe56d922a2","subtype":"command","commandType":"auto","position":27.75,"command":"test = get_stat_grp_sat(small_g, predictions)\ntest\n# 12.28 minutes for 5 groups ho_grps small_grps for predictions, 3 mem\n# 14.22 minutes for 5 groups he_grps cons_grps for predictions, 3 mem\n# 17.63 minutes for 5 groups ho_grps cons_grps for predictions, 5 mem\n# 15.38 minutes for 5 groups he_grps cons_grps for predictions, 5 mem\n# 18.76 minutes for 5 groups ho_grps cons_grps for predictions, 7 mem\n# 26.52 minutes for 5 groups he_grps cons_grps for predictions, 7 mem","commandVersion":0,"state":"error","results":null,"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.481983634308E12,"submitTime":1.481983634308E12,"finishTime":1.481983824953E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"9be86e56-c810-4b5c-8a68-518a8b8c7d0d"},{"version":"CommandV1","origId":1146,"guid":"4094af33-c2c2-45ea-884f-40cb5df73bb5","subtype":"command","commandType":"auto","position":27.84375,"command":"test","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">78</span><span class=\"ansired\">]: </span>\n[([(4677916, 58),\n   (947986, 26),\n   (2373298, 88),\n   (5220892, 70),\n   (5773564, 8),\n   (3638828, 66),\n   (19530572, 25)],\n  0.24588101432426049,\n  0.24848513115750828,\n  0.34009675959003999,\n  0.22972732704405077),\n ([(17623894, 81),\n   (2777500, 69),\n   (1187356, 47),\n   (7266748, 50),\n   (14664796, 4),\n   (9390172, 11),\n   (15033244, 33)],\n  0.25343700484525328,\n  0.24540621585801375,\n  0.31505442323341709,\n  0.22318465269093221),\n ([(14140, 39),\n   (11606314, 51),\n   (14548444, 40),\n   (3178066, 84),\n   (19530572, 25),\n   (17156668, 12),\n   (2874460, 77)],\n  0.18182881656836661,\n  0.17882679398035803,\n  0.30752685567542154,\n  0.056119097141492151),\n ([(947986, 26),\n   (4377340, 35),\n   (14819932, 67),\n   (3178066, 84),\n   (11085356, 89),\n   (5337244, 74),\n   (334108, 31)],\n  0.1347312616380634,\n  0.14951756036150074,\n  0.31735938825962567,\n  0.14126373278805773),\n ([(343804, 34),\n   (3216850, 14),\n   (5388754, 6),\n   (13304730, 20),\n   (9681052, 96),\n   (2835676, 46),\n   (12612274, 5)],\n  0.25209896414334715,\n  0.21410223594953212,\n  0.37692180909271278,\n  0.24903539331151794)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.481947999763E12,"submitTime":1.481947997771E12,"finishTime":1.481947999839E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"1d548f33-02eb-40da-971a-165708e460f1"},{"version":"CommandV1","origId":1057,"guid":"e1fc7064-0a05-4405-a63b-cfd7ef07a4a3","subtype":"command","commandType":"auto","position":28.03125,"command":"print test","commandVersion":0,"state":"error","results":null,"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;test&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-13-f3e50bd42a45&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">print</span> test<span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;test&apos; is not defined\n</div>","workflows":[],"startTime":1.481927477305E12,"submitTime":1.481927477305E12,"finishTime":1.481931540871E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"26e965b3-1989-4297-976a-c1f59ab97f6d"},{"version":"CommandV1","origId":1043,"guid":"b23b7bcc-880f-4774-9671-26e2bf68ad45","subtype":"command","commandType":"auto","position":28.125,"command":"## TEST CODE\n\nfiltergrptest = [u[0] for u in grptest]\nfilteredpredstest = test.filter(lambda p: p[0] in filtergrptest)\n# filteredpredstest.take(3)\nprint \"Least misery:\", get_grp_sat(grptest, get_lm_recolist(grptest, test, 20), filteredpredstest)\nprint \"Most pleasure:\", get_grp_sat(grptest, get_mp_recolist(grptest, test, 20), filteredpredstest)\nprint \"Average:\", get_grp_sat(grptest, get_ave_recolist(grptest, test, 20), filteredpredstest)\nprint \"Multiply:\", get_grp_sat(grptest, get_mul_recolist(grptest, test, 20), filteredpredstest)\n\n\n# ho_3members\n# Least misery: 0.705580204341\n# Most pleasure: 0.608114688563\n# Average: 0.720762866939\n\n# he_3members\n# Least misery: 0.40638466995\n# Most pleasure: 0.346111858355\n# Average: 0.447872295007\n# Multiply: 0.429937042367\n# 3.7 min","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Least misery: 0.40638466995\nMost pleasure: 0.346111858355\nAverage: 0.447872295007\nMultiply: 0.429937042367\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-56-9f1e6dc065fb&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">6</span>\n<span class=\"ansiyellow\">    print &quot;Average:&quot;, get_grp_sat(grptest, get_ave_recolist(grptest, test, 20), filteredpredstest</span>\n<span class=\"ansigrey\">        ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> invalid syntax\n\n</div>","workflows":[],"startTime":1.481915039989E12,"submitTime":1.481915038452E12,"finishTime":1.481915132893E12,"collapsed":true,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d1e59074-000e-4888-96fd-0e7407312dcc"}],"dashboards":[],"guid":"7bd8c264-71c0-477d-b74d-8d0ce6637f3d","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"f8374683-f914-43a7-82d8-7a4bfe3a6567","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>