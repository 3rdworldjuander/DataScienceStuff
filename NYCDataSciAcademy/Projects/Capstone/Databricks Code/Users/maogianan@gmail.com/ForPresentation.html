<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>maogianan@gmail.com / ForPresentation - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/latest.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"spark_heap_memory":21145,"instance_type_id":"r3.xlarge","node_type_id":"r3.xlarge","description":"r3.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26432,"memory_mb":31232,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":true},{"spark_heap_memory":46131,"instance_type_id":"r3.2xlarge","node_type_id":"r3.2xlarge","description":"r3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":57664,"memory_mb":62464,"category":"Memory Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":96102,"instance_type_id":"r3.4xlarge","node_type_id":"r3.4xlarge","description":"r3.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":120128,"memory_mb":124928,"category":"Memory Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":196044,"instance_type_id":"r3.8xlarge","node_type_id":"r3.8xlarge","description":"r3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":245056,"memory_mb":249856,"category":"Memory Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":8448,"instance_type_id":"c3.2xlarge","node_type_id":"c3.2xlarge","description":"c3.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":10560,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":20736,"instance_type_id":"c3.4xlarge","node_type_id":"c3.4xlarge","description":"c3.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":25920,"memory_mb":30720,"category":"Compute Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":45312,"instance_type_id":"c3.8xlarge","node_type_id":"c3.8xlarge","description":"c3.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":56640,"memory_mb":61440,"category":"Compute Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":21145,"instance_type_id":"i2.xlarge","node_type_id":"i2.xlarge","description":"i2.xlarge (beta)","support_cluster_tags":true,"container_memory_mb":26432,"memory_mb":31232,"category":"Storage Optimized","num_cores":4.0,"support_ebs_volumes":true},{"spark_heap_memory":46131,"instance_type_id":"i2.2xlarge","node_type_id":"i2.2xlarge","description":"i2.2xlarge (beta)","support_cluster_tags":true,"container_memory_mb":57664,"memory_mb":62464,"category":"Storage Optimized","num_cores":8.0,"support_ebs_volumes":true},{"spark_heap_memory":96102,"instance_type_id":"i2.4xlarge","node_type_id":"i2.4xlarge","description":"i2.4xlarge (beta)","support_cluster_tags":true,"container_memory_mb":120128,"memory_mb":124928,"category":"Storage Optimized","num_cores":16.0,"support_ebs_volumes":true},{"spark_heap_memory":196044,"instance_type_id":"i2.8xlarge","node_type_id":"i2.8xlarge","description":"i2.8xlarge (beta)","support_cluster_tags":true,"container_memory_mb":245056,"memory_mb":249856,"category":"Storage Optimized","num_cores":32.0,"support_ebs_volumes":true},{"spark_heap_memory":23800,"instance_type_id":"r3.2xlarge","node_type_id":"memory-optimized","description":"Memory Optimized","support_cluster_tags":false,"container_memory_mb":28000,"memory_mb":30720,"category":"Memory Optimized","num_cores":4.0,"support_ebs_volumes":false},{"spark_heap_memory":9702,"instance_type_id":"c3.4xlarge","node_type_id":"compute-optimized","description":"Compute Optimized","support_cluster_tags":false,"container_memory_mb":12128,"memory_mb":15360,"category":"Compute Optimized","num_cores":8.0,"support_ebs_volumes":false}],"default_node_type_id":"memory-optimized"},"enableThirdPartyApplicationsUI":false,"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":0,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-east-1e","isDefault":true},{"id":"us-east-1b","isDefault":false},{"id":"us-east-1c","isDefault":false},{"id":"us-east-1d","isDefault":false}],"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":false,"enableMaxConcurrentRuns":false,"enableJobAclsConfig":true,"enableFullTextSearch":true,"enableElasticSparkUI":true,"enableNewClustersCreate":false,"clusters":true,"allowRunOnPendingClusters":true,"applications":false,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":true,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"sparkVersions":[{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0 RC5 (Scala 2.11)","packageLabel":"spark-image-20833506f690f3a49c53fb08837cb9b98f7f2e15380f1fb26efc158d953e94c8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-6c2dd678fff350c03ba0e945bab52d0080cd857a39c99a22131b3e824bb8096f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-4f22a8d3016bc3dce9e839b10418815a7d28afff3a027b43bd2b041c42b2a89d","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-596490ba3e0ca4b62abb048923fc70de84e319cf527bb7a5a8f609bbf780bed8","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":true},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0 RC5 (Scala 2.10)","packageLabel":"spark-image-f430b652793cbc27e454dc629737ae2bc04b75a1fecd68a5d90c44eca7543756","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":false,"enableFeedback":true,"enableClusterAutoScaling":true,"enableUserVisibleDefaultTags":false,"defaultNumWorkers":8,"serverContinuationTimeoutMillis":10000,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":true,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"p2.8xlarge":16,"r3.8xlarge":8,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"c4.8xlarge":4,"r3.xlarge":1,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":true,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":false,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.33.220","accountsLimit":-1,"enableX509Authentication":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":30,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"enableCssTransitions":true,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":true,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"support@databricks.com","enableDriverLogsUI":true,"disableLegacyDashboards":false,"enableWorkspaceAclsConfig":true,"dropzoneMaxFileSize":4096,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"defaultSparkVersion":{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-4f22a8d3016bc3dce9e839b10418815a7d28afff3a027b43bd2b041c42b2a89d","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":true,"enableMountAclsConfig":false,"useDevTierHomePage":false,"enablePublishHub":false,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":true,"enableClusterAclsByTier":true,"databricksDocsBaseUrl":"https://docs.databricks.com/","disallowAddingAdmins":false,"enableSparkConfUI":true,"featureTier":"UNKNOWN_TIER","enableOrgSwitcherUI":false,"clustersLimit":-1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":false,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"enableDashboardViews":false,"dbcFeedbackURL":"http://feedback.databricks.com/forums/263785-product-feedback","enableMountAclService":true,"enableWorkspaceAcls":true,"maxClusterTagKeyLength":127,"gitHash":"8719902676c4e1cefbc31755c1726979bdf286d0","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":false,"enableDebugUI":false,"allowNonAdminUsers":true,"enableSingleSignOnByTier":true,"enableJobsRetryOnTimeout":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSparkPackages":true,"dynamicSparkVersions":true,"enableNotebookHistoryUI":true,"showDebugCounters":false,"enableInstanceProfilesUI":true,"enableFolderHtmlExport":true,"enableSparkVersionsUI":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"upgradeURL":"","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":true,"enableTerminal":false,"defaultMemoryPerContainerMB":28000,"enablePresenceUI":true,"accounts":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"enableNewClustersGet":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":1234,"name":"maogianan@gmail.com / ForPresentation","language":"python","commands":[{"version":"CommandV1","origId":1236,"guid":"043c56fd-52d4-428c-bf9e-21a639364e3c","subtype":"command","commandType":"auto","position":1.0,"command":"%md #Model to recommended playlist  \nThis notebook details all functions to come up with a recommended playlist for the group.  \n\nInputs are:  \n- model: retireved from AWS S3  \n- Users list: list of users in the group  \n\nOther variables needed:  \n- list_length: integer number of songs of the recommended playlist. ","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d1976c2a-0ae7-491a-81eb-8fbff2d8a3ba"},{"version":"CommandV1","origId":1237,"guid":"bd4082fe-d851-4771-ab52-1c9d89fe53e7","subtype":"command","commandType":"auto","position":1.5,"command":"# Run this cell to setup data path\nimport os\nfrom pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\n# Configuration\ndatapath = os.getcwd()\nif datapath.find('databricks') != -1:\n  ACCESS_KEY =   \"AKIAIUOEPA4PZ2RIARXA\"\n  SECRET_KEY = \"vkVXcbOTaWx7k4HnrvaeWblnbZkedeYEX5DVtNN+\"\n  ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n  AWS_BUCKET_NAME = \"dummyvarsmillionsong\"\n  MOUNT_NAME = \"/S3/\"\n  datapath = \"s3a://%s:%s@%s/\" %(ACCESS_KEY, SECRET_KEY, AWS_BUCKET_NAME)\nelse:\n  print \"Error! Not on databricks server\"\n  \n# Load and parse ratings\ntripletsRDD = sc.textFile('processed_data/ratings_full.txt')\nratings = tripletsRDD.map(lambda l: l.split(','))\\\n    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n  \n# Load spotify user data\nspotify_triplets = sc.textFile(\"/mnt/S3/processed_data/josh_ratings.txt,/mnt/S3/processed_data/oamar_ratings.txt,/mnt/S3/processed_data/james_ratings.txt\")\n\nspotify_ratings = spotify_triplets.map(lambda l: l.split(','))\\\n    .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n  \n# HACK: Add Spotify users to see history and recommendations\nratings = ratings.union(spotify_ratings)\nusers = ratings.map(lambda x: x.user).distinct()\n\n# Sanity check\nprint users.count()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">1019320\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482084863447E12,"submitTime":1.48208486234E12,"finishTime":1.482084924548E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"be21fe05-bc33-469f-8c13-ea606ae3a1f8"},{"version":"CommandV1","origId":1238,"guid":"515dc526-63bc-4f1f-82cb-12e0191b12f3","subtype":"command","commandType":"auto","position":2.0,"command":"# Mounting S3 bucket\nimport os\nfrom pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\n# S3 bucket mounting \nACCESS_KEY = \"AKIAIUOEPA4PZ2RIARXA\"\nSECRET_KEY = \"vkVXcbOTaWx7k4HnrvaeWblnbZkedeYEX5DVtNN+\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"dummyvarsmillionsong\"\nMOUNT_NAME = \"/S3/\"\n# Mount the S3 bucket\n# dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n# Display contents of bucket\ndisplay(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/mnt/S3/mapping/","mapping/",0.0],["dbfs:/mnt/S3/models/","models/",0.0],["dbfs:/mnt/S3/processed_data/","processed_data/",0.0],["dbfs:/mnt/S3/raw_data/","raw_data/",0.0],["dbfs:/mnt/S3/results/","results/",0.0]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482084924552E12,"submitTime":1.48208487524E12,"finishTime":1.482084925988E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"796c9718-9a41-40b7-81f8-8c06fce988cd"},{"version":"CommandV1","origId":1239,"guid":"b9de0168-c941-43de-bf0a-55925e4b74df","subtype":"command","commandType":"auto","position":3.0,"command":"# Load saved model from S3\nfrom pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n\nmodel = MatrixFactorizationModel.load(sc, \"/mnt/S3/models/good_model\")\nspmodel = MatrixFactorizationModel.load(sc, \"/mnt/S3/models/spotify_model\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.482084926307E12,"submitTime":1.48208492245E12,"finishTime":1.482084935039E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"81b02d27-df79-4a3f-8a67-5295f4ac76a8"},{"version":"CommandV1","origId":1240,"guid":"285ffff4-cf63-4253-b4b0-bee18119eaa3","subtype":"command","commandType":"auto","position":5.0,"command":"from __builtin__ import max\n\ndef sort_and_normalize(product_rating_pairs):\n  \"\"\"\n  Normalize ratings of a (user, (product, rating)) RDD\n  \"\"\"\n  max_rating = max(map(lambda x: x[1], product_rating_pairs))\n  product_rating_pairs_normalized = map(lambda x: (x[0], x[1]/max_rating) , product_rating_pairs)\n  sorted_normalized_pairs = sorted(product_rating_pairs_normalized, key=lambda y: y[1], reverse=True)\n  return sorted_normalized_pairs\n\n\ndef get_predictions_for_users(grp, model):\n  \"\"\"\n  Returns a normalized RDD of the model prediction of all user(in grp), product and ratings\n  Output is [user, longlist of songs and normalized ratings] for n-users\n  \"\"\"\n  # Get all users in the group\n#   users_ingroup = [u[0] for u in grp]\n  users_ingroup = grp\n  # Get all products in the model\n  products_list = model.productFeatures().map(lambda p: p[0]) # using saved model\n  # Create all possible user product pairs\n  users_ingroup_product_pairs = sc.parallelize(users_ingroup).cartesian(products_list)\n  # Get predictions for each user\n  predictions_keys = model.predictAll(users_ingroup_product_pairs).map(lambda x: (x[0], (x[1], x[2])))\n  predictions_by_user = predictions_keys.groupByKey().map(lambda x : (x[0], sort_and_normalize(x[1])))\n  return predictions_by_user\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482084925991E12,"submitTime":1.482084881487E12,"finishTime":1.48208492606E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"06b2c78c-5c54-47d8-977b-11331a98d26c"},{"version":"CommandV1","origId":1241,"guid":"ee0203f5-b603-450e-a89d-3b5be9c156d1","subtype":"command","commandType":"auto","position":6.0,"command":"# Aggregation Strategies\n\ndef get_lm_recolist(grp, predictions_for_users, list_length):\n  \"\"\" \n  Least Misery Agg strategy\n  Output is a playlist\n  \"\"\"\n#   filtergrp = [u[0] for u in grp]\n  filtergrp = grp\n  lm_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(min).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return lm_recolist\n\ndef get_mp_recolist(grp, predictions_for_users, list_length):\n  \"\"\"\n  Most Pleasure Agg strategy\n  Output is a playlist\n  \"\"\"\n#   filtergrp = [u[0] for u in grp]\n  filtergrp = grp\n  mp_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(max).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return mp_recolist\n\nfrom operator import add\ndef get_ave_recolist(grp, predictions_for_users, list_length):\n  \"\"\"\n  Average Agg strategy\n  Output is a playlist\n  \"\"\"\n#   filtergrp = [u[0] for u in grp]\n  filtergrp = grp\n  ave_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(add).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return ave_recolist\n\nfrom operator import mul\ndef get_mul_recolist(grp, predictions_for_users, list_length):\n  \"\"\"\n  Multiply Aggregation strategy\n  Output is a playlist\n  \"\"\"\n#   filtergrp = [u[0] for u in grp]\n  filtergrp = grp\n  ave_recolist = predictions_for_users.filter(lambda p: p[0] in filtergrp) \\\n    .flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])) \\\n    .reduceByKey(mul).sortBy(lambda p: p[1], ascending = False).take(list_length)\n  return ave_recolist\n\n\n# Average Aggregation Strategy on Normalized Ratings!!!\n\ndef get_real_ave_recolist(grp, predictions_for_users, list_length):\n  filtergrp = grp\n  sumCount = predictions_for_users.flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])).combineByKey(lambda value: (value, 1),\n                               lambda x, value: (x[0] + value, x[1] + 1),\n                               lambda x, y: (x[0] + y[0], x[1] + y[1]))\n\n  average_agg = sumCount.map(lambda (label, (value_sum, count)): (label, value_sum / count))\n\n  return average_agg.sortBy(lambda p: p[1], ascending = False).take(list_length)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482084926063E12,"submitTime":1.482084883468E12,"finishTime":1.482084926157E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"bdfc7130-4691-416d-bbca-29ef1150214b"},{"version":"CommandV1","origId":1242,"guid":"6aa1bee1-d814-4c49-92b6-7edc36352692","subtype":"command","commandType":"auto","position":10.0,"command":"# Helper functions\n# Load corresponding files\nimport os\nimport json\n\ndef get_song_info(song_json):\n  \"\"\"\n  Return the tuple (song_id, spotify_id, track_name, artist_name) given a json format.\n  song_id is of the form 'SOXXX' as defined by million song dataset.\n  \"\"\"\n  \n  try:\n    song_dict = json.loads(song_json)\n    song = song_dict['response']['songs'][0]\n\n    song_id = song['id']\n    artist_name = song['artist_name']\n    track_name = song['title']\n\n    spotify_id = ''\n    for t in song['tracks']:\n      foreign_id = t['foreign_id']\n      if foreign_id.startswith('spotify'):\n        spotify_id = foreign_id\n\n    return (song_id, spotify_id, artist_name, track_name)\n  except:\n    return None\n  \ndef get_song_path(song_id):\n  \"\"\"Get a song_id's json file path\"\"\"\n  \n  path_to_mappings = \"/mnt/S3/mapping/\"\n  folder_name = song_id[2:4]\n  return os.path.join(path_to_mappings, folder_name, song_id + '.json')\n\ndef get_song_details(rec_ids):\n  \"\"\"\n  Return song info for each song in rec_ids.\n  rec_ids contains the integers specifying the song index used in the recommendation algorithm.\n  \"\"\"\n  \n  # Map rec_ids to song_ids\n  # Note that we change the order so we can lookup song id from recommended id\n  kaggle_songs_path = os.path.join(datapath, \"raw_data/fromKaggle/kaggle_songs.txt\")\n  song_ids_list = sc.textFile(kaggle_songs_path)\\\n    .map(lambda x: x.split())\\\n    .map(lambda x: (int(x[1]), x[0]))\\\n    .filter(lambda x: x[0] in rec_ids)\\\n    .collect()\n  \n  song_ids_mapping = dict(song_ids_list)\n  \n  song_ids = [song_ids_mapping[id] for id in rec_ids]\n  \n  # Map songs_ids to info\n  path_to_song_files = [get_song_path(song_id) for song_id in song_ids]\n  \n  song_files = sc.textFile(','.join(path_to_song_files))\n  song_details = song_files.map(lambda x: get_song_info(x)).collect()\n  return song_details\n\ndef get_spotify_ids(rec_ids):\n  \"\"\"\n  Return the spotify ids of songs in rec_ids.\n  rec_ids contains the integers specifying the song index used in the recommendation algorithm.\n  \"\"\"\n  song_details = get_song_details(rec_ids)\n  spotify_ids = [d[1] for d in song_details if d]\n  return spotify_ids\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.48208492616E12,"submitTime":1.482084889244E12,"finishTime":1.482084926281E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"51c7fe2d-5e99-4bed-bfb6-f360a4b6f4b3"},{"version":"CommandV1","origId":1243,"guid":"be9f744a-689c-4201-bea5-1682bb160694","subtype":"command","commandType":"auto","position":11.0,"command":"# GET SPOTIFY PLAYLIST\ndef get_final_sp_list(grp, model, list_length, aggr = get_ave_recolist):\n  \"\"\"\n  Function for getting final output\n  Aggregation strategies, aggr:\n  get_ave_recolist, Average (default)\n  get_lm_recolist, Least Misery\n  get_mp_recolist, Most Pleasure\n  get_mul_recolist, Multiply\n  \"\"\"\n  predictions_all = get_predictions_for_users(grp, model)\n  #Employ aggregation strategy use switching here:\n  p_list = aggr(grp, predictions_all, list_length)\n  # End of all agg strategies\n  \n  p_list = map(lambda p: p[0], p_list)\n  \n  #Convert to Spotify\n  final_list = get_spotify_ids(p_list)\n  return final_list","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">NameError</span>: name &apos;get_ave_recolist&apos; is not defined","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-2-bb537cffadea&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansired\"># GET SPOTIFY PLAYLIST</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 2</span><span class=\"ansiyellow\"> </span><span class=\"ansigreen\">def</span> get_final_sp_list<span class=\"ansiyellow\">(</span>grp<span class=\"ansiyellow\">,</span> model<span class=\"ansiyellow\">,</span> list_length<span class=\"ansiyellow\">,</span> aggr <span class=\"ansiyellow\">=</span> get_ave_recolist<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      3</span>   &quot;&quot;&quot;\n<span class=\"ansigreen\">      4</span>   Function <span class=\"ansigreen\">for</span> getting final output<span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      5</span>   Aggregation strategies<span class=\"ansiyellow\">,</span> aggr<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">NameError</span>: name &apos;get_ave_recolist&apos; is not defined\n</div>","workflows":[],"startTime":1.482084926283E12,"submitTime":1.482084895052E12,"finishTime":1.482084926304E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"3dbebedd-56d2-4dc6-8966-434552297812"},{"version":"CommandV1","origId":1244,"guid":"be894bf8-9ea6-49e1-95e7-0f9ef515fea4","subtype":"command","commandType":"auto","position":12.0,"command":"# test_grp = [(84250, 722), (92650, 722), (193650, 722)]\ntest_grp = [84250, 92650, 193650] # Homogenous group\nus = [2000000 , 3000000 , 4000000 ] # Josh, James, Oamar. Probably heterogenous 2000000 (josh), 3000000 (oamar), 4000000\npredictions_all = get_predictions_for_users(test_grp, model)\npredictions_us = get_predictions_for_users(us, spmodel)\n# p_list = get_ave_recolist(test_grp, predictions_all, 20)\n# test = get_final_sp_list(test_grp, model, 50)\n# ustest = get_final_sp_list(us, spmodel, 50)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1.482084958216E12,"submitTime":1.482084957069E12,"finishTime":1.482084983833E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4f4b384d-a0b9-402e-bbf7-06c25a591f78"},{"version":"CommandV1","origId":1250,"guid":"e73681ef-fca9-4fd6-a928-08a88ee256e1","subtype":"command","commandType":"auto","position":15.5,"command":"predictions_all.flatMapValues(lambda p: p).take(5)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">42</span><span class=\"ansired\">]: </span>\n[(84250, (220530, 1.0)),\n (84250, (145141, 0.9780677366386423)),\n (84250, (351667, 0.9727521780060772)),\n (84250, (97516, 0.9719027272951625)),\n (84250, (367757, 0.9667758280031769))]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">TypeError</span>: &apos;int&apos; object is not iterable","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">TypeError</span>                                 Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-39-6171a6be3ddf&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>get_predictions_for_users<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">84250</span><span class=\"ansiyellow\">,</span> model<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-3-c50b6714b125&gt;</span> in <span class=\"ansicyan\">get_predictions_for_users</span><span class=\"ansiblue\">(grp, model)</span>\n<span class=\"ansigreen\">     22</span>   products_list <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>productFeatures<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> p<span class=\"ansiyellow\">:</span> p<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span> <span class=\"ansired\"># using saved model</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     23</span>   <span class=\"ansired\"># Create all possible user product pairs</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 24</span><span class=\"ansiyellow\">   </span>users_ingroup_product_pairs <span class=\"ansiyellow\">=</span> sc<span class=\"ansiyellow\">.</span>parallelize<span class=\"ansiyellow\">(</span>users_ingroup<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>cartesian<span class=\"ansiyellow\">(</span>products_list<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     25</span>   <span class=\"ansired\"># Get predictions for each user</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     26</span>   predictions_keys <span class=\"ansiyellow\">=</span> model<span class=\"ansiyellow\">.</span>predictAll<span class=\"ansiyellow\">(</span>users_ingroup_product_pairs<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">2</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/context.pyc</span> in <span class=\"ansicyan\">parallelize</span><span class=\"ansiblue\">(self, c, numSlices)</span>\n<span class=\"ansigreen\">    430</span>             <span class=\"ansired\"># Make sure we distribute data evenly if it&apos;s smaller than self.batchSize</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    431</span>             <span class=\"ansigreen\">if</span> <span class=\"ansiblue\">&quot;__len__&quot;</span> <span class=\"ansigreen\">not</span> <span class=\"ansigreen\">in</span> dir<span class=\"ansiyellow\">(</span>c<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 432</span><span class=\"ansiyellow\">                 </span>c <span class=\"ansiyellow\">=</span> list<span class=\"ansiyellow\">(</span>c<span class=\"ansiyellow\">)</span>    <span class=\"ansired\"># Make it a list so we can compute its length</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    433</span>             batchSize <span class=\"ansiyellow\">=</span> max<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">,</span> min<span class=\"ansiyellow\">(</span>len<span class=\"ansiyellow\">(</span>c<span class=\"ansiyellow\">)</span> <span class=\"ansiyellow\">//</span> numSlices<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_batchSize <span class=\"ansigreen\">or</span> <span class=\"ansicyan\">1024</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    434</span>             serializer <span class=\"ansiyellow\">=</span> BatchedSerializer<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_unbatched_serializer<span class=\"ansiyellow\">,</span> batchSize<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">TypeError</span>: &apos;int&apos; object is not iterable\n</div>","workflows":[],"startTime":1.482086817652E12,"submitTime":1.482086816555E12,"finishTime":1.48208681896E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"f8ed2f0f-321c-4129-9b7b-2f45a8e8ead6"},{"version":"CommandV1","origId":1251,"guid":"709addd9-dd6d-4489-a88c-cd120c91e292","subtype":"command","commandType":"auto","position":17.25,"command":"pre = [137832, 145141, 220530, 327643, 351667, 367757, 48460]\nzip(get_song_details(pre), pre)","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">56</span><span class=\"ansired\">]: </span>\n[((u&apos;SOVUROV13167713FDB&apos;,\n   u&apos;spotify:track:3UQ3V6jVo47xJv09x8QaO0&apos;,\n   u&apos;Lifehouse&apos;,\n   u&apos;Whatever It Takes&apos;),\n  137832),\n ((u&apos;SOJDJWL12AB0186A16&apos;,\n   u&apos;spotify:track:2ISdZkOX4tRb7N2ou62N7S&apos;,\n   u&apos;Lifehouse&apos;,\n   u&apos;From Where You Are&apos;),\n  145141),\n ((u&apos;SOODTBU12AB0186A1F&apos;,\n   u&apos;spotify:track:7GVDugddNyO1D2PhxiSQED&apos;,\n   u&apos;Lifehouse&apos;,\n   u&apos;Falling In&apos;),\n  220530),\n (None, 327643),\n ((u&apos;SOTNURA1315CD446F5&apos;,\n   u&apos;spotify:track:6dwf86ERsHePlIgP4jc6Rf&apos;,\n   u&apos;Lifehouse&apos;,\n   u&apos;Storm&apos;),\n  351667),\n ((u&apos;SOAEXQL13D7EA4AD49&apos;,\n   u&apos;spotify:track:44bm3BYQtzzLLUnLUb44vV&apos;,\n   u&apos;Lifehouse&apos;,\n   u&apos;All In&apos;),\n  367757),\n (None, 48460)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2635.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2635.0 (TID 101610, 10.172.225.32): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","error":"<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;ipython-input-53-a73bd9a64162&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>get_song_details<span class=\"ansiyellow\">(</span><span class=\"ansicyan\">137832</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">&lt;ipython-input-5-cfa1e2f406ed&gt;</span> in <span class=\"ansicyan\">get_song_details</span><span class=\"ansiblue\">(rec_ids)</span>\n<span class=\"ansigreen\">     44</span>   <span class=\"ansired\"># Note that we change the order so we can lookup song id from recommended id</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     45</span>   kaggle_songs_path <span class=\"ansiyellow\">=</span> os<span class=\"ansiyellow\">.</span>path<span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>datapath<span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&quot;raw_data/fromKaggle/kaggle_songs.txt&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 46</span><span class=\"ansiyellow\">   </span>song_ids_list <span class=\"ansiyellow\">=</span> sc<span class=\"ansiyellow\">.</span>textFile<span class=\"ansiyellow\">(</span>kaggle_songs_path<span class=\"ansiyellow\">)</span>    <span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> x<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>    <span class=\"ansiyellow\">.</span>map<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> <span class=\"ansiyellow\">(</span>int<span class=\"ansiyellow\">(</span>x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span>    <span class=\"ansiyellow\">.</span>filter<span class=\"ansiyellow\">(</span><span class=\"ansigreen\">lambda</span> x<span class=\"ansiyellow\">:</span> x<span class=\"ansiyellow\">[</span><span class=\"ansicyan\">0</span><span class=\"ansiyellow\">]</span> <span class=\"ansigreen\">in</span> rec_ids<span class=\"ansiyellow\">)</span>    <span class=\"ansiyellow\">.</span>collect<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     47</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     48</span>   song_ids_mapping <span class=\"ansiyellow\">=</span> dict<span class=\"ansiyellow\">(</span>song_ids_list<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/rdd.pyc</span> in <span class=\"ansicyan\">collect</span><span class=\"ansiblue\">(self)</span>\n<span class=\"ansigreen\">    774</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">    775</span>         <span class=\"ansigreen\">with</span> SCCallSiteSync<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>context<span class=\"ansiyellow\">)</span> <span class=\"ansigreen\">as</span> css<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 776</span><span class=\"ansiyellow\">             </span>port <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>ctx<span class=\"ansiyellow\">.</span>_jvm<span class=\"ansiyellow\">.</span>PythonRDD<span class=\"ansiyellow\">.</span>collectAndServe<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jrdd<span class=\"ansiyellow\">.</span>rdd<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    777</span>         <span class=\"ansigreen\">return</span> list<span class=\"ansiyellow\">(</span>_load_from_socket<span class=\"ansiyellow\">(</span>port<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>_jrdd_deserializer<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    778</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1131</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1132</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1133</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1134</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1135</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    317</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    318</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 319</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    320</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    321</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2635.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2635.0 (TID 101610, 10.172.225.32): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;&lt;ipython-input-5-cfa1e2f406ed&gt;&quot;, line 46, in &lt;lambda&gt;\nTypeError: argument of type &apos;int&apos; is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1891)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1904)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1917)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:911)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor225.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 172, in main\n    process()\n  File &quot;/databricks/spark/python/pyspark/worker.py&quot;, line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File &quot;/databricks/spark/python/pyspark/serializers.py&quot;, line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &quot;&lt;ipython-input-5-cfa1e2f406ed&gt;&quot;, line 46, in &lt;lambda&gt;\nTypeError: argument of type &apos;int&apos; is not iterable\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.&lt;init&gt;(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n</div>","workflows":[],"startTime":1.482088055617E12,"submitTime":1.482088054514E12,"finishTime":1.482088057627E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"75ae6f8a-cd3f-4c1f-89e2-b497374449fe"},{"version":"CommandV1","origId":1248,"guid":"7be06bda-8827-4ba0-909e-3d05b9ee7bb3","subtype":"command","commandType":"auto","position":19.0,"command":"rlistOf84250 = predictions_all.filter(lambda p: p[0] == 84250).flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])).collect()\n\nrlistOf92650 = predictions_all.filter(lambda p: p[0] == 92650).flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])).collect()\n\nrlistOf193650 = predictions_all.filter(lambda p: p[0] == 193650).flatMapValues(lambda p: p).map(lambda x: (x[1][0], x[1][1])).collect()","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> unexpected EOF while parsing","error":"<div class=\"ansiout\"><span class=\"ansicyan\">  File </span><span class=\"ansigreen\">&quot;&lt;ipython-input-19-2234e4230080&gt;&quot;</span><span class=\"ansicyan\">, line </span><span class=\"ansigreen\">1</span>\n<span class=\"ansiyellow\">    predictions_all.filter(lambda p: p[0] == 84250).take(4#.filter(lambda p: p[1][0] in (map(lambda p: p[0],plist))).collect()</span>\n<span class=\"ansigrey\">                                                                                                                              ^</span>\n<span class=\"ansired\">SyntaxError</span><span class=\"ansired\">:</span> unexpected EOF while parsing\n\n</div>","workflows":[],"startTime":1.482086898406E12,"submitTime":1.482086897294E12,"finishTime":1.482086906541E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"80114517-1832-45f9-9945-0c53fbe5e57c"},{"version":"CommandV1","origId":1249,"guid":"ed60d801-37da-49c7-b912-f4938103a96f","subtype":"command","commandType":"auto","position":20.0,"command":"","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">52</span><span class=\"ansired\">]: </span>\n[(220530, 1.0),\n (145141, 0.9780677366386423),\n (351667, 0.9727521780060772),\n (97516, 0.9719027272951625),\n (367757, 0.9667758280031769),\n (137832, 0.9534056321668898),\n (193516, 0.937742526701741),\n (327643, 0.9365229714729165),\n (336193, 0.9322002781308232),\n (48460, 0.92773999116322),\n (224405, 0.8926754473645735),\n (151198, 0.8924967132016508),\n (321542, 0.8838020611582019),\n (169295, 0.882798830222716),\n (175259, 0.8738187393470233),\n (90379, 0.8695832198943071),\n (335366, 0.8684013811314765),\n (234319, 0.8665930603546383),\n (189681, 0.8631903855839633),\n (361683, 0.8486835843479928)]\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1.482087204737E12,"submitTime":1.482087203623E12,"finishTime":1.482087204807E12,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"d5791d94-1a92-4cff-b119-90e6fcd074e6"}],"dashboards":[],"guid":"3318161b-66a3-4aa1-9834-9f7bba573a9f","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
<script>var tableOfContentsCell = {"version":"CommandV1","origId":0,"guid":"d422633d-b780-418d-8618-6385fd6377de","subtype":"command","commandType":"auto","position":0.0,"command":"%md [&lsaquo; Back to Table of Contents](../index.html)","commandVersion":1,"state":"finished","results":{"type":"raw","data":"","arguments":{}},"errorSummary":null,"error":null,"workflows":[],"startTime":0.0,"submitTime":0.0,"finishTime":0.0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{}};</script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>