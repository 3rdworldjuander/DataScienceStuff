library("MASS", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
install.packages(c("MASS", "ggplot2", "igraph", "Hmisc"))
install.packages(c("dplyr", "lubridate", "knitr", "igraph"))
install.packages("circlize")
install.packages("googleVis")
install.packages(c("Shiny", "shinythemes"))
install.packages(c("reshape", "reshape2", "DT"))
install.packages("FeatureHashing")
library(h2o)
library(h2oEnsemble)
library(h2oEnsemble)
library(h2o)
library(h2o)
kd_h2o<-h2o.init(nthreads = -1, max_mem_size = "8g")
# Installation of H2O-Ensemble, does not work on Kaggle cloud
# install.packages("https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.8.tar.gz", repos = NULL)
library(h2oEnsemble)
# Locally start jar and then use this line
# kd_h2o<-h2o.init(ip = "localhost", port = 54323 ,nthreads = -1, max_mem_size = "10g")
#### Loading data ####
#Reading Data, old school read.csv. Using fread is faster.
set.seed(12345)
train<-read.csv('train.csv')
test<-read.csv('test.csv')
train.index <- train[,1]
test.index <- test[,1]
train.loss <- train[,ncol(train)]
#### Pre-processing dataset ####
#Combining train and test data for joint pre-processing
bulk <- rbind(train[,-ncol(train)], test)
bulk$id <- NULL
#Converting categories to numeric
#this is done by first splitting the binary level, multi-level, and
#continuous variables
#colnames(all.train)
bin.bulk <- bulk[,1:72]
cat.bulk <- bulk[,73:116]
cont.bulk <- bulk[,117:130]
#Combine levels
#Combining multiple levels using combine.levels
#minimum frequency = minlev
temp <- sapply(cat.bulk, combine.levels, minlev = 0.02)
temp <- as.data.frame(temp)
str(temp)
#Column bind binary and reduced categorical variables
# comb.train <- cbind(bin.train, cat.train)
comb.bulk <- cbind(bin.bulk, temp)
#Dummify all factor variables
dmy <- dummyVars(" ~ .", data = comb.bulk, fullRank=T)
temp <- as.data.frame(predict(dmy, newdata = comb.bulk))
dim(temp)
#Combine dummified with cont vars
bulk <- cbind(temp, cont.bulk)
dim(bulk)
#Split pre-cprocessed dataset into train and test
train.e = bulk[1:nrow(train),]
test.e = bulk[(nrow(train)+1):nrow(bulk),]
#Re-attach index
train.e <- cbind(train.index, train.e)
test.e <- cbind(test.index, test.e)
#Re-attach loss
train.e$loss <- train.loss
#Pre-processed data for training and validation
train <- train.e
test <- test.e
#### Start of H2O part ####
#Removing index column
train <- train[,-1]
test_label <- test[,1]
test <- test[,-1]
#Getting index of test subset
index <- sample(1:(dim(train)[1]), 0.2*dim(train)[1], replace=FALSE)
#Creating training=train_frame and test=valid_frame subsets
train_frame<-train[-index,]
valid_frame<-train[index,]
#Separating loss variable from test set. valid_predict has NO LOSS variable
valid_predict<-valid_frame[,-ncol(valid_frame)]
valid_loss<-valid_frame[,ncol(valid_frame)]
#Log transform
train_frame[,ncol(train_frame)]<-log(train_frame[,ncol(train_frame)])
valid_frame[,ncol(train_frame)]<-log(valid_frame[,ncol(valid_frame)])
# load H2o data frame // validate that H2O flow looses all continous data
train_frame.hex<-as.h2o(train_frame)
valid_frame.hex<-as.h2o(valid_frame)
valid_predict.hex<-as.h2o(valid_predict)
test.hex<-as.h2o(test)
setwd("~/Documents/NYCDataScience/Projects/ML")
library(h2o)
kd_h2o<-h2o.init(nthreads = -1, max_mem_size = "8g")
# Installation of H2O-Ensemble, does not work on Kaggle cloud
# install.packages("https://h2o-release.s3.amazonaws.com/h2o-ensemble/R/h2oEnsemble_0.1.8.tar.gz", repos = NULL)
library(h2oEnsemble)
# Locally start jar and then use this line
# kd_h2o<-h2o.init(ip = "localhost", port = 54323 ,nthreads = -1, max_mem_size = "10g")
#### Loading data ####
#Reading Data, old school read.csv. Using fread is faster.
set.seed(12345)
train<-read.csv('train.csv')
test<-read.csv('test.csv')
train.index <- train[,1]
test.index <- test[,1]
train.loss <- train[,ncol(train)]
#### Pre-processing dataset ####
#Combining train and test data for joint pre-processing
bulk <- rbind(train[,-ncol(train)], test)
bulk$id <- NULL
#Converting categories to numeric
#this is done by first splitting the binary level, multi-level, and
#continuous variables
#colnames(all.train)
bin.bulk <- bulk[,1:72]
cat.bulk <- bulk[,73:116]
cont.bulk <- bulk[,117:130]
#Combine levels
#Combining multiple levels using combine.levels
#minimum frequency = minlev
temp <- sapply(cat.bulk, combine.levels, minlev = 0.02)
temp <- as.data.frame(temp)
str(temp)
#Column bind binary and reduced categorical variables
# comb.train <- cbind(bin.train, cat.train)
comb.bulk <- cbind(bin.bulk, temp)
#Dummify all factor variables
dmy <- dummyVars(" ~ .", data = comb.bulk, fullRank=T)
temp <- as.data.frame(predict(dmy, newdata = comb.bulk))
dim(temp)
#Combine dummified with cont vars
bulk <- cbind(temp, cont.bulk)
dim(bulk)
#Split pre-cprocessed dataset into train and test
train.e = bulk[1:nrow(train),]
test.e = bulk[(nrow(train)+1):nrow(bulk),]
#Re-attach index
train.e <- cbind(train.index, train.e)
test.e <- cbind(test.index, test.e)
#Re-attach loss
train.e$loss <- train.loss
#Pre-processed data for training and validation
train <- train.e
test <- test.e
#### Start of H2O part ####
#Removing index column
train <- train[,-1]
test_label <- test[,1]
test <- test[,-1]
#Getting index of test subset
index <- sample(1:(dim(train)[1]), 0.2*dim(train)[1], replace=FALSE)
#Creating training=train_frame and test=valid_frame subsets
train_frame<-train[-index,]
valid_frame<-train[index,]
#Separating loss variable from test set. valid_predict has NO LOSS variable
valid_predict<-valid_frame[,-ncol(valid_frame)]
valid_loss<-valid_frame[,ncol(valid_frame)]
#Log transform
train_frame[,ncol(train_frame)]<-log(train_frame[,ncol(train_frame)])
valid_frame[,ncol(train_frame)]<-log(valid_frame[,ncol(valid_frame)])
# load H2o data frame // validate that H2O flow looses all continous data
train_frame.hex<-as.h2o(train_frame)
valid_frame.hex<-as.h2o(valid_frame)
valid_predict.hex<-as.h2o(valid_predict)
test.hex<-as.h2o(test)
